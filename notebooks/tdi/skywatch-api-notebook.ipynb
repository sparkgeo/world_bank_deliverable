{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: run the below 2 cells first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\" #Put skywatch api key here\n",
    "headers = {'Content-Type': 'application/json', 'x-api-key': api_key} #Used to construct queries. Do not change\n",
    "\n",
    "#Path to put all outputs\n",
    "out_path = Path(\"../data/raw/planetscope/\")\n",
    "#AOI geometry file used to query API\n",
    "aoi_file = Path(\"../data/processed/Beitbridge_PS_AOI_Final.gpkg\")\n",
    "aoi_data = gpd.read_file(aoi_file)\n",
    "aoi_json = json.loads(aoi_data.to_json())\n",
    "\n",
    "o_start_date = \"2023-01-01\" #Overall start date for query\n",
    "o_end_date = \"2023-03-27\" #Overall end date for query\n",
    "aoi_name = \"BeitBridge\" #Name for AOI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_interval = 90\n",
    "\n",
    "o_start_datetime = datetime.datetime.strptime(o_start_date, \"%Y-%m-%d\")\n",
    "o_end_datetime = datetime.datetime.strptime(o_end_date, \"%Y-%m-%d\")\n",
    "num_intervals = math.floor((o_end_datetime - o_start_datetime).days / date_interval)\n",
    "\n",
    "date_list = []\n",
    "\n",
    "if num_intervals > 0:\n",
    "    starting_date = None\n",
    "    for i in range(0, num_intervals):\n",
    "        if not starting_date:\n",
    "            starting_date = o_start_datetime\n",
    "        ending_date = starting_date + datetime.timedelta(days=date_interval)\n",
    "\n",
    "        date_list.append({\"start_date\": datetime.datetime.strftime(starting_date, \"%Y-%m-%d\"), \"end_date\": datetime.datetime.strftime(ending_date, \"%Y-%m-%d\")})\n",
    "        starting_date = ending_date + datetime.timedelta(days=1)\n",
    "\n",
    "    final_start_date = ending_date + datetime.timedelta(days=1)\n",
    "    date_list.append({\"start_date\": datetime.datetime.strftime(final_start_date, \"%Y-%m-%d\"), \"end_date\": o_end_date})\n",
    "\n",
    "else:\n",
    "    date_list.append({\"start_date\": o_start_date, \"end_date\": o_end_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sat_type(sat_id, source):\n",
    "    dove_r_list = [\"1047\", \"1057\", \"1058\", \"1059\", \"105a\", \"105b\", \"105d\", \"105e\", \"105f\", \"1060\", \"1061\", \"1062\", \"1063\", \"1064\", \"1065\", \"1066\", \"1067\", \"1068\", \"1069\", \"106a\", \"106b\", \"106c\", \"106d\", \"106e\", \"106f\"]\n",
    "\n",
    "    sat_parse = sat_id.split('_')[-1]\n",
    "    if source == \"PlanetScope-SuperDove\":\n",
    "        sat_type = 'superdove'\n",
    "    if source == \"PlanetScope\":\n",
    "        sat_type = 'dove-c'\n",
    "    \n",
    "    if any(sat in sat_parse for sat in dove_r_list):\n",
    "        sat_type = \"dove-r\"\n",
    "\n",
    "    return sat_type\n",
    "\n",
    "def create_search_query(start_date, end_date, aoi_json):\n",
    "    query = {\n",
    "            \"location\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": aoi_json[\"features\"][0][\"geometry\"][\"coordinates\"][0]\n",
    "            },\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"resolution\": [\"medium\"],\n",
    "            \"coverage\": 50.0,\n",
    "            \"interval_length\": 7,\n",
    "            \"order_by\": \"date\"\n",
    "        }\n",
    "    \n",
    "    return query\n",
    "\n",
    "def search_catalog(query, headers):\n",
    "    search_url = \"https://api.skywatch.co/earthcache/archive/search\"\n",
    "    search_resp = requests.post(url=search_url, data=json.dumps(query), headers=headers).json()\n",
    "    search_id = search_resp['data']['id']\n",
    "    return search_id\n",
    "\n",
    "def get_search_results(search_id, headers):\n",
    "    search_url = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results\"\n",
    "    search_resp = requests.get(url=search_url, headers=headers)\n",
    "\n",
    "    if search_resp.status_code == 202:\n",
    "        while search_resp.status_code == 202:\n",
    "            print(\"Search still processing. Waiting 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            search_resp = requests.get(url=search_url, headers=headers)\n",
    "    \n",
    "    return search_resp.json()\n",
    "\n",
    "def parse_results(search_resp, headers, search_id, df_list):\n",
    "    for item in search_resp[\"data\"]:\n",
    "        datestr = item['start_time'].split(\"T\")[0]\n",
    "        date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n",
    "        year = date_obj.year\n",
    "        month = date_obj.month\n",
    "        isodate = date_obj.isocalendar()\n",
    "        if isodate[0] == (year - 1) and isodate[1] == 52:\n",
    "            week = 1\n",
    "        else:\n",
    "            week = isodate[1]\n",
    "        \n",
    "        sat_type = get_sat_type(item['product_name'], item['source'])\n",
    "        item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n",
    "                                \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n",
    "                                item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n",
    "                                \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n",
    "        df_list.append(item_df)\n",
    "    try:\n",
    "        cursor = search_resp['pagination']['cursor']['next']\n",
    "    except:\n",
    "        cursor = None\n",
    "    \n",
    "    while cursor:\n",
    "        search_url3 = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results?cursor={cursor}\"\n",
    "        search_resp_2 = requests.get(url=search_url3, headers=headers).json()\n",
    "        for item in search_resp_2[\"data\"]:\n",
    "            datestr = item['start_time'].split(\"T\")[0]\n",
    "            date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n",
    "            year = date_obj.year\n",
    "            month = date_obj.month\n",
    "            isodate = date_obj.isocalendar()\n",
    "            if isodate[0] == (year - 1) and isodate[1] == 52:\n",
    "                week = 1\n",
    "            else:\n",
    "                week = isodate[1]\n",
    "\n",
    "            sat_type = get_sat_type(item['product_name'], item['source'])\n",
    "            item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n",
    "                                    \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n",
    "                                    item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n",
    "                                    \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n",
    "            df_list.append(item_df)\n",
    "        \n",
    "        try:\n",
    "            cursor = search_resp_2['pagination']['cursor']['next']\n",
    "        except:\n",
    "            cursor = None\n",
    "    \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for entry in date_list:\n",
    "    start_date = entry['start_date']\n",
    "    end_date = entry['end_date']\n",
    "    print(f\"running search query for date range: {start_date} to {end_date}\")\n",
    "    query = create_search_query(start_date, end_date, aoi_json)\n",
    "    search_id = search_catalog(query, headers)\n",
    "    search_resp = get_search_results(search_id, headers)\n",
    "\n",
    "    df_list = parse_results(search_resp, headers, search_id, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = pd.concat(df_list)\n",
    "search_df.reset_index(drop=True, inplace=True)\n",
    "search_df['year'] = search_df['year'].astype('category')\n",
    "search_df['month'] = search_df['month'].astype('category')\n",
    "search_df['week'] = search_df['week'].astype('category')\n",
    "search_df[\"preview_html\"] = search_df.apply(lambda x: f'<a href=\\\"{x[\"preview\"]}\\\">Preview</a>', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}.pkl')\n",
    "search_df.to_pickle(search_pickle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter query Dataframe. Continue from here if using previous query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}.pkl')\n",
    "\n",
    "with open(search_pickle, \"rb\") as f:\n",
    "    search_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates(df):\n",
    "    top_coverage = df.nlargest(1,'aoi_coverage')[\"aoi_coverage\"].tolist()[0]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        df['cover_percentmax'] = df.apply(lambda row: (row[\"aoi_coverage\"] / top_coverage), axis=1)\n",
    "        \n",
    "    df = df.loc[(df[\"cover_percentmax\"]>=0.9)]\n",
    "\n",
    "    if 'superdove' in df['sat_type'].unique() or 'dove-r' in df['sat_type']:\n",
    "        df = df.loc[(df['sat_type'] == 'superdove') | (df['sat_type'] == 'dove-r')]\n",
    "\n",
    "    df = df.nlargest(1,'aoi_coverage')\n",
    "    select_id = df['id'].values[0]\n",
    "\n",
    "    return select_id\n",
    "\n",
    "def filter_dataframe(df, cloud_cover_thresh, coverage_thresh):\n",
    "    filtered_df = df.copy(deep=True)\n",
    "    filtered_df = filtered_df.loc[(search_df[\"cloud_cover\"]<=cloud_cover_thresh) & (filtered_df[\"aoi_coverage\"] >= coverage_thresh)]\n",
    "\n",
    "    id_list = []\n",
    "    unique_dates = filtered_df.datestr.unique().tolist()\n",
    "    for date_ in unique_dates:\n",
    "        filtered_df2 = filtered_df.loc[filtered_df[\"datestr\"]==date_]\n",
    "        if len(filtered_df2) > 1:\n",
    "            select_id = filter_dates(filtered_df2)\n",
    "        else:\n",
    "            select_id = filtered_df2['id'].values[0]\n",
    "        \n",
    "        id_list.append(select_id)\n",
    "\n",
    "    filtered_df = filtered_df[filtered_df['id'].isin(id_list)]\n",
    "\n",
    "    filtered_df.reset_index(inplace=True, drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "cc_thresh = 2.0\n",
    "coverage_thresh = 80.0\n",
    "filtered_search = filter_dataframe(search_df, cc_thresh, coverage_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\n",
    "filtered_search.to_pickle(filter_search_pickle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating graphs to explore query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\n",
    "with open(filter_search_pickle, \"rb\") as f:\n",
    "    filtered_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_month_cc_graph(df, target_year, output_graph):\n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "    min_date = target_df.iloc[0]['datestr']\n",
    "    max_date = target_df.iloc[-1]['datestr']\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    month_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['month'].value_counts()\n",
    "    month_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['month'].value_counts()\n",
    "    month_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['month'].value_counts()\n",
    "    month_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['month'].value_counts()\n",
    "    month_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['month'].value_counts()\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Month - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Monthly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_month_cc_graph(filtered_search, 2021, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_week_cc_graph(df, target_year, output_graph):\n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "    min_date = target_df.iloc[0]['datestr']\n",
    "    max_date = target_df.iloc[-1]['datestr']\n",
    "    week_df = pd.DataFrame()\n",
    "    week_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['week'].value_counts()\n",
    "    week_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['week'].value_counts()\n",
    "    week_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['week'].value_counts()\n",
    "    week_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['week'].value_counts()\n",
    "    week_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['week'].value_counts()\n",
    "\n",
    "    week_df.reset_index(inplace=True)\n",
    "    week_df.rename(columns={'index': 'week'}, inplace=True)\n",
    "    week_df.sort_values('week', inplace=True)\n",
    "    week_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    week_fig = px.bar(week_df, x='week', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    week_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Week - {min_date} to {max_date} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Week of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Weekly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        week_fig.write_image(out_png)\n",
    "        week_fig.write_html(out_html)\n",
    "\n",
    "    week_fig.show()\n",
    "\n",
    "create_week_cc_graph(filtered_search, 2020, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_preview_graph(df, output_graph):\n",
    "    layout = go.Layout(title=f'PlanetScope Images - {o_start_date} to {o_end_date} - {aoi_name}', xaxis=dict(title=\"Image Date\"), yaxis=dict(title=\"Cloud Cover Percentage\"))#, autosize=False, width=1200, height=510)\n",
    "\n",
    "    fig = go.Figure(layout=layout)\n",
    "\n",
    "    h_template='Image Date: %{customdata[0]}<br>%{customdata[3]}<br>Cloud Cover: %{customdata[2]}%<br>Image Source: %{customdata[1]}'\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=df['cloud_cover'],\n",
    "        x=df['date'],\n",
    "        mode='markers',\n",
    "        orientation='h',\n",
    "        marker=dict(\n",
    "            color='red'),\n",
    "        customdata=np.stack((df['datestr'], df['source'], df['cloud_cover'], df['preview_html']), axis=-1),\n",
    "        hovertemplate=h_template\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title={'x': 0.5, 'xanchor': 'center'})\n",
    "\n",
    "    if output_graph:\n",
    "        out_html = out_path.joinpath(f\"PlanetScope_Image_Previews_{o_start_date}_to_{o_end_date}_{aoi_name}.html\")\n",
    "        fig.write_html(out_html)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "create_image_preview_graph(filtered_search, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_year_cc_graph(df, output_graph):\n",
    "    target_df = df\n",
    "    min_year = target_df.iloc[0]['year']\n",
    "    max_year = target_df.iloc[-1]['year']\n",
    "    avg_price = target_df[\"cost\"].mean()\n",
    "    year_df = pd.DataFrame()\n",
    "    year_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['year'].value_counts()\n",
    "    year_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['year'].value_counts()\n",
    "    year_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['year'].value_counts()\n",
    "    year_df[\"0% clouds cost\"] = year_df.apply(lambda row: round((row[\"0% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\"<=2% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=2% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\"<=5% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=5% clouds\"] * avg_price), 2), axis=1)\n",
    "\n",
    "    year_df.reset_index(inplace=True)\n",
    "    year_df.rename(columns={'index': 'year'}, inplace=True)\n",
    "    year_df.sort_values('year', inplace=True)\n",
    "    year_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    h_template='Image Count 0% Clouds: %{customdata[0]} Estimated Cost: %{customdata[3]}<br>Image Count <=2% Clouds: %{customdata[1]} Estimated Cost: %{customdata[4]}<br>Image Count <=5% Clouds: %{customdata[2]} Estimated Cost: %{customdata[5]}'\n",
    "    year_fig = px.bar(year_df, x='year', y=['0% clouds', '<=2% clouds', '<=5% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\"}, custom_data=np.stack((year_df['0% clouds'], year_df['<=2% clouds'], year_df['<=5% clouds'], year_df['0% clouds cost'], year_df['<=2% clouds cost'], year_df['<=5% clouds cost'])))\n",
    "    year_fig.update_layout(title_text=f\"PlanetScope Annual Image Counts and Cost - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n",
    "    year_fig.update_traces(hovertemplate=h_template)\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Annual_Image_Count_and_Cost_{min_year}_to_{max_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        year_fig.write_image(out_png)\n",
    "        year_fig.write_html(out_html)\n",
    "\n",
    "    year_fig.show()\n",
    "\n",
    "    return year_df\n",
    "\n",
    "year_df = create_year_cc_graph(filtered_search, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_month_multiyear_graph(df, output_graph):\n",
    "    \n",
    "    min_year = df.iloc[0]['year']\n",
    "    max_year = df.iloc[-1]['year']\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    col_list = []\n",
    "    for year in df.year.unique().tolist():\n",
    "        target_df = df.loc[df[\"year\"] == year]\n",
    "        col_name = f\"{year}_counts\"\n",
    "        month_df[col_name] = target_df.loc[target_df['cloud_cover'] == 0]['month'].value_counts()\n",
    "        col_list.append(col_name)\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    month_fig = px.bar(month_df, x='month', y=col_list, barmode='group')\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and Month - 0% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Image_Count_Annual_Monthly_0%CC_{min_year}_to_{max_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_month_multiyear_graph(filtered_search, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dove_monthly_graph(df, target_year, cc_thresh, output_graph):\n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "    min_date = target_df.iloc[0]['datestr']\n",
    "    max_date = target_df.iloc[-1]['datestr']\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    col_list = []\n",
    "    for datasource in df.sat_type.unique().tolist():\n",
    "        if datasource == \"dove-c\":\n",
    "            col_name = \"Dove-Classic\"\n",
    "        elif datasource == \"dove-r\":\n",
    "            col_name = \"Dove-R\"\n",
    "        elif datasource == \"superdove\":\n",
    "            col_name = \"SuperDove\"\n",
    "        \n",
    "        month_df[col_name] = target_df.loc[(target_df['cloud_cover'] == cc_thresh) & (target_df['sat_type'] == datasource)]['month'].value_counts()\n",
    "        col_list.append(col_name)\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    month_fig = px.bar(month_df, x='month', y=col_list)\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Month and By Satellite Type - {cc_thresh}% Cloud Cover - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Image_Count_Monthly_{cc_thresh}%CC_SatelliteType_{target_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_dove_monthly_graph(filtered_search, 2021, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dove_year_graph(df, cc_thresh, output_graph):\n",
    "    target_df = df\n",
    "    min_year = df.iloc[0]['year']\n",
    "    max_year = df.iloc[-1]['year']\n",
    "    # avg_price = target_df[\"cost\"].mean()\n",
    "    year_df = pd.DataFrame()\n",
    "    year_df[\"Dove-Classic\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-c\")]['year'].value_counts()\n",
    "    year_df[\"Dove-R\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-r\")]['year'].value_counts()\n",
    "    year_df[\"SuperDove\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"superdove\")]['year'].value_counts()\n",
    "    \n",
    "    year_df.reset_index(inplace=True)\n",
    "    year_df.rename(columns={'index': 'year'}, inplace=True)\n",
    "    year_df.sort_values('year', inplace=True)\n",
    "    year_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    year_fig = px.bar(year_df, x='year', y=[\"Dove-Classic\", \"Dove-R\", \"SuperDove\"])\n",
    "    year_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and By Satellite Type - {cc_thresh}% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Annual_Image_Count_{cc_thresh}_%CC_SatelliteType_{min_year}_to_{max_year}_{aoi_name}_filtered\"\n",
    "    if output_graph:\n",
    "        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n",
    "        year_fig.write_image(out_png)\n",
    "        year_fig.write_html(out_html)\n",
    "\n",
    "    year_fig.show()\n",
    "\n",
    "    return year_df\n",
    "\n",
    "year_df = create_dove_year_graph(filtered_search, 0, False)\n",
    "# year_df = create_dove_year_graph(search_df, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\n",
    "with open(filter_search_pickle, \"rb\") as f:\n",
    "    filtered_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final image download selection criteria\n",
    "max_cc = 0.0\n",
    "# start_date_download = o_start_date\n",
    "# end_date_download = o_end_date\n",
    "start_date_download = \"2023-01-01\"\n",
    "end_date_download = \"2023-03-27\"\n",
    "start_date_final = datetime.datetime.strptime(start_date_download, \"%Y-%m-%d\")\n",
    "end_date_final = datetime.datetime.strptime(end_date_download, \"%Y-%m-%d\")\n",
    "out_path = out_path.joinpath(out_path, f\"outputs_{aoi_name}_{start_date_download}_to_{end_date_download}\")\n",
    "out_path.mkdir(exist_ok=True)\n",
    "sat_list = ['superdove']\n",
    "filtered_download_pickle = out_path.joinpath(f'search_df_{start_date_download}_to_{end_date_download}_download.pkl')\n",
    "filtered_download_resp_pickle = out_path.joinpath(f'search_df_{start_date_download}_to_{end_date_download}_resp.pkl')\n",
    "\n",
    "pl_resp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Filter results down to final download list based on above criteria\n",
    "filtered_search_final = filtered_search.copy(deep=True)\n",
    "filtered_search_final = filtered_search_final.loc[(filtered_search_final['date']>=start_date_final) & (filtered_search_final['date']<=end_date_final) & (filtered_search_final['cloud_cover']<=max_cc)& (filtered_search_final['sat_type'].isin(sat_list))]\n",
    "\n",
    "filtered_search_final['pl_id'] = \"\"\n",
    "filtered_search_final['pl_status'] = \"\"\n",
    "\n",
    "filtered_search_final.to_pickle(filtered_download_pickle)\n",
    "\n",
    "total_cost = filtered_search_final['cost'].sum()\n",
    "num_images = len(filtered_search_final)\n",
    "print(f\"Total number of images to download: {num_images}. Total cost: ${total_cost} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query endpoint to get different output types and build dictionary of selectable output types\n",
    "output_id_url = f\"https://api.skywatch.co/earthcache/outputs\"\n",
    "output_id_resp = requests.get(url=output_id_url, headers=headers).json()\n",
    "output_type_dict = {}\n",
    "for output in output_id_resp['data']:\n",
    "    output_name = output['name']\n",
    "    output_id = output['id']\n",
    "    output_type_dict[output_name] = output_id\n",
    "\n",
    "output_type_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_id = output_type_dict['All Optical Bands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_query(pipeline_name, search_id, image_id, output_id):\n",
    "    query = {\n",
    "            \"name\": pipeline_name,\n",
    "            \"search_id\": search_id,\n",
    "            \"search_results\": image_id,\n",
    "            \"output\": {\n",
    "                \"id\": output_id,\n",
    "                \"format\": \"geotiff\",\n",
    "                \"mosaic\": \"off\"\n",
    "            },\n",
    "            }\n",
    "    \n",
    "    return query\n",
    "\n",
    "def post_pipeline(query, headers):\n",
    "    pipeline_url = \"https://api.skywatch.co/earthcache/pipelines\"\n",
    "    pipeline_resp = requests.post(url=pipeline_url, data=json.dumps(query), headers=headers).json()\n",
    "    pipeline_id = pipeline_resp['data']['id']\n",
    "    return pipeline_id\n",
    "\n",
    "def get_pipeline(pipeline_id):\n",
    "    pipeline_get_url = f\"https://api.skywatch.co/earthcache/interval_results?pipeline_id={pipeline_id}\"\n",
    "    pipeline_get_resp = requests.get(url=pipeline_get_url, headers=headers)#.json()\n",
    "\n",
    "    return pipeline_get_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in filtered_search_final.iterrows():\n",
    "    time.sleep(1)\n",
    "    search_id = row[\"search_id\"]\n",
    "    image_id = row[\"id\"]\n",
    "    product_name = row[\"product_name\"]\n",
    "    print(f\"creating download pipeline for image: {product_name}\")\n",
    "    pipeline_name = f\"{aoi_name}_{product_name}\"\n",
    "    pl_query = create_pipeline_query(pipeline_name, search_id, image_id, output_id)\n",
    "    pl_id = post_pipeline(pl_query, headers)\n",
    "    filtered_search_final.loc[index, 'pl_id'] = pl_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_search_final.to_pickle(filtered_download_pickle)\n",
    "\n",
    "filter_search_csv = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filtered_download_pickle.exists():\n",
    "    with open(filtered_download_pickle, 'rb') as f:\n",
    "        filtered_search_final = pickle.load(f)\n",
    "if filtered_download_resp_pickle.exists():\n",
    "    with open(filtered_download_resp_pickle, 'rb') as f:\n",
    "        pl_resp_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline(pl_id):\n",
    "    pl_resp = get_pipeline(pl_id)\n",
    "    pl_resp_json = pl_resp.json()\n",
    "    pl_status = pl_resp_json['data'][0]['status']\n",
    "    return pl_status, pl_resp_json\n",
    "\n",
    "num_iterations = 1\n",
    "results_processing = True\n",
    "while results_processing:\n",
    "    status_list = []\n",
    "    print('Checking status of image download pipelines.')\n",
    "    for index, row in filtered_search_final.iterrows():\n",
    "        time.sleep(1)\n",
    "        pl_status = row['pl_status']\n",
    "        pl_id = row['pl_id']\n",
    "        if pl_status != 'complete':\n",
    "            pl_status, pl_resp_json = query_pipeline(pl_id)\n",
    "            filtered_search_final.loc[index, 'pl_status'] = pl_resp_json['data'][0]['status']\n",
    "            pl_resp_dict[pl_id] = pl_resp_json\n",
    "        \n",
    "        status_list.append(pl_status)\n",
    "\n",
    "    # if 'retrieving' not in set(status_list) and 'processing' not in set(status_list):\n",
    "    filtered_search_final.to_pickle(filtered_download_pickle)\n",
    "    with open(filtered_download_resp_pickle, 'wb') as f:\n",
    "        pickle.dump(pl_resp_dict, f)\n",
    "    if 'complete' in set(status_list) and len(set(status_list)) == 1:\n",
    "        results_processing = False\n",
    "        print('All image pipelines are finished and ready for download!')\n",
    "    else:\n",
    "        wait_time = 1800 / num_iterations\n",
    "        if wait_time < 60:\n",
    "            wait_time = 60\n",
    "        wait_time_mins = round((wait_time / 60), 0)\n",
    "        print(f\"Results still pending for some items. Waiting for {wait_time_mins} mins and trying again.\")\n",
    "        num_iterations += 2\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, out_name):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=None):\n",
    "                f.write(chunk)\n",
    "\n",
    "for index, row in filtered_search_final.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    pl_id = row['pl_id']\n",
    "    pl_resp_json = pl_resp_dict[pl_id]\n",
    "    results = pl_resp_json['data'][0]['results'][0]\n",
    "    download_dict = {'image': {'url': results['analytics_url'], 'extension': 'analytic.tif'},\n",
    "                    'metadata': {'url': results['metadata_url'], 'extension': 'metadata.json'},\n",
    "                    'cloud_mask': {'url': results['raster_files'][0]['uri'], 'extension': 'mask.tif'}}\n",
    "\n",
    "    out_basepath = out_path.joinpath(product_name)\n",
    "    out_basepath.mkdir(exist_ok=True)\n",
    "    print(f'downloading all files for image: {product_name}')\n",
    "    for key, value in download_dict.items():\n",
    "        dl_url = value['url']\n",
    "        extension = value['extension']\n",
    "        out_fname = out_basepath.joinpath(f'{product_name}_{extension}')\n",
    "        download_file(dl_url, out_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb-skywatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b205056744192b51d7fcf647b8488be922fc7e73105947b81a7c7b6d880083f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
