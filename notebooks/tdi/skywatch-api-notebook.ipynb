{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skywatch API Notebook\n",
    "This notebook is used to query and download PlanetScope imagery using the Skywatch EarthCache API within a specified AOI and date range. <br> The notebook performs the following tasks:\n",
    "1. Queries the Skywatch API for the area and time of interest. Returns dataframe with all images meeting the specified criteria.\n",
    "2. Filters results to select the \"best\" image per day, if there are multiple images on a given day.\n",
    "3. Visualize results in a variety of graphs.\n",
    "4. Download selected images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Paths and Query Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\" # Put skywatch api key here\n",
    "headers = {'Content-Type': 'application/json', 'x-api-key': api_key} # Used to construct queries. Do not change\n",
    "\n",
    "# Overall path to put all outputs\n",
    "out_path = Path(\"../../data/tdi_demo_files\")\n",
    "out_path = out_path.resolve()\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to put search queries\n",
    "outpath_search = out_path.joinpath('search_queries')\n",
    "outpath_search.mkdir(exist_ok=True)\n",
    "\n",
    "aoi_file = Path(\"../../data/processed/Beitbridge_PS_AOI_Final.gpkg\") # AOI geometry file used to query API\n",
    "aoi_file = aoi_file.resolve()\n",
    "aoi_data = gpd.read_file(aoi_file)\n",
    "aoi_json = json.loads(aoi_data.to_json())\n",
    "\n",
    "o_start_date = \"2018-01-01\" # Overall start date for query\n",
    "o_end_date = \"2023-03-31\" # Overall end date for query\n",
    "aoi_name = \"BeitBridge\" # Name for AOI\n",
    "\n",
    "cc_thresh = 25.0 # Maximum allowable image cloud cover percentage\n",
    "coverage_thresh = 80.0 # Minimum AOI image coverage percentage\n",
    "\n",
    "output_graphs = True # Option to output graphs in Stage 3 to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    print(\"No API key found. Beware that querying and downloading imagery will fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Search API\n",
    "This part of the notebook queries the Earthcache API for the specified AOI and TOI. Because of limitations of the API, long date ranges can fail to return all results. To get around this, the overall use-specified date range is split into multiple more manageable queries, with the results combined at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of days to split up into separate queries. Skywatch API fails to return full results if too wide of a date range is used, so this is used as a workaround\n",
    "# The separate queries are combined at the end of the process.\n",
    "date_interval = 90\n",
    "\n",
    "o_start_datetime = datetime.datetime.strptime(o_start_date, \"%Y-%m-%d\")\n",
    "o_end_datetime = datetime.datetime.strptime(o_end_date, \"%Y-%m-%d\")\n",
    "num_intervals = math.floor((o_end_datetime - o_start_datetime).days / date_interval)\n",
    "\n",
    "date_list = []\n",
    "\n",
    "if num_intervals > 0:\n",
    "    starting_date = None\n",
    "    for i in range(0, num_intervals):\n",
    "        if not starting_date:\n",
    "            starting_date = o_start_datetime\n",
    "        ending_date = starting_date + datetime.timedelta(days=date_interval)\n",
    "\n",
    "        if ending_date > o_end_datetime:\n",
    "            ending_date = o_end_datetime\n",
    "\n",
    "        date_list.append({\"start_date\": datetime.datetime.strftime(starting_date, \"%Y-%m-%d\"), \"end_date\": datetime.datetime.strftime(ending_date, \"%Y-%m-%d\")})\n",
    "        starting_date = ending_date + datetime.timedelta(days=1)\n",
    "\n",
    "    if ending_date < o_end_datetime:\n",
    "        final_start_date = ending_date + datetime.timedelta(days=1)\n",
    "        date_list.append({\"start_date\": datetime.datetime.strftime(final_start_date, \"%Y-%m-%d\"), \"end_date\": o_end_date})\n",
    "\n",
    "else:\n",
    "    date_list.append({\"start_date\": o_start_date, \"end_date\": o_end_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sat_type(sat_id, source):\n",
    "    \"\"\"\n",
    "    Parses the type of Dove satellite from the image source returned from the Skywatch API as well as the satellite ID string.\n",
    "    This is performed because the Skywatch API does not currently make any distinction between the Dove-Classic and Dove-R\n",
    "    satellite platforms.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    sat_id: str\n",
    "        identification string parsed from the image name\n",
    "    source: str\n",
    "        image source returned from the Skywatch API\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    sat_type: str\n",
    "        satellite type for the input image\n",
    "\n",
    "    \"\"\"\n",
    "    # List of dove-r satellite ID strings\n",
    "    dove_r_list = [\"1047\", \"1057\", \"1058\", \"1059\", \"105a\", \"105b\", \"105d\", \"105e\", \"105f\", \"1060\", \"1061\", \"1062\", \"1063\", \"1064\", \"1065\", \"1066\", \"1067\", \"1068\", \"1069\", \"106a\", \"106b\", \"106c\", \"106d\", \"106e\", \"106f\"]\n",
    "\n",
    "    sat_parse = sat_id.split('_')[-1]\n",
    "    if source == \"PlanetScope-SuperDove\":\n",
    "        sat_type = 'superdove'\n",
    "    if source == \"PlanetScope\":\n",
    "        sat_type = 'dove-c'\n",
    "    \n",
    "    if any(sat in sat_parse for sat in dove_r_list):\n",
    "        sat_type = \"dove-r\"\n",
    "\n",
    "    return sat_type\n",
    "\n",
    "def create_search_query(start_date, end_date, aoi_json):\n",
    "    \"\"\"\n",
    "    Creates a json query to use for searching the Skywatch API.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    start_date: str\n",
    "        starting date string for query\n",
    "    end_date: str\n",
    "        ending date string for query\n",
    "    aoi_json: geojson\n",
    "        geojson geometry for query AOI\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    query: dict\n",
    "        formatted query dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    query = {\n",
    "            \"location\": {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": aoi_json[\"features\"][0][\"geometry\"][\"coordinates\"][0]\n",
    "            },\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"resolution\": [\"medium\"], # medium resolution = planetscope in skywatch\n",
    "            \"coverage\": 50.0, # default mimimum aoi coverage of 50% for initial query, with a more stringent filter applied later\n",
    "            \"interval_length\": 7,\n",
    "            \"order_by\": \"date\"\n",
    "        }\n",
    "    \n",
    "    return query\n",
    "\n",
    "def search_catalog(query, headers):\n",
    "    \"\"\"\n",
    "    Runs a POST request to create search query.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    query: dict\n",
    "        formatted query dictionary\n",
    "    headers: dict\n",
    "        headers for query\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    search_id: str\n",
    "        id for search query\n",
    "\n",
    "    \"\"\"\n",
    "    search_url = \"https://api.skywatch.co/earthcache/archive/search\"\n",
    "    search_resp = requests.post(url=search_url, data=json.dumps(query), headers=headers).json()\n",
    "    search_id = search_resp['data']['id']\n",
    "    \n",
    "    return search_id\n",
    "\n",
    "def get_search_results(search_id, headers):\n",
    "    \"\"\"\n",
    "    GET request to retrieve search results for a specified query\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    search_id: str\n",
    "        id for search query\n",
    "    headers: dict\n",
    "        headers for query\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    search_resp_json: json\n",
    "        json containing search results for query\n",
    "\n",
    "    \"\"\"\n",
    "    search_url = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results\"\n",
    "    search_resp = requests.get(url=search_url, headers=headers)\n",
    "\n",
    "    # Query results take a bit of time to populate after the initial POST request. This loop\n",
    "    # repeats the get request every 5 seconds until results are available.\n",
    "    if search_resp.status_code == 202:\n",
    "        while search_resp.status_code == 202:\n",
    "            print(\"Search still processing. Waiting 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            search_resp = requests.get(url=search_url, headers=headers)\n",
    "    \n",
    "    search_resp_json = search_resp.json()\n",
    "    \n",
    "    return search_resp_json\n",
    "\n",
    "def parse_results(search_resp, headers, search_id, df_list):\n",
    "    \"\"\"\n",
    "    Parses through returned query results and creates pandas dataframes from them.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    search_resp: json\n",
    "        query results json\n",
    "    headers: dict\n",
    "        headers for query\n",
    "    search_id: str\n",
    "        id for search query\n",
    "    df_list: list\n",
    "        list of search dataframes created from earlier search results\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    df_list: list\n",
    "        list of search dataframes from previous and current search results\n",
    "    \"\"\"\n",
    "    # Parses through search json results and populates dataframe with select image attributes\n",
    "    for item in search_resp[\"data\"]:\n",
    "        datestr = item['start_time'].split(\"T\")[0]\n",
    "        date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n",
    "        year = date_obj.year\n",
    "        month = date_obj.month\n",
    "        isodate = date_obj.isocalendar()\n",
    "        if isodate[0] == (year - 1) and isodate[1] == 52:\n",
    "            week = 1\n",
    "        else:\n",
    "            week = isodate[1]\n",
    "        \n",
    "        sat_type = get_sat_type(item['product_name'], item['source'])\n",
    "        item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n",
    "                                \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n",
    "                                item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n",
    "                                \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n",
    "        df_list.append(item_df)\n",
    "    \n",
    "    # Check for pagination of search results\n",
    "    try:\n",
    "        cursor = search_resp['pagination']['cursor']['next']\n",
    "    except:\n",
    "        cursor = None\n",
    "    \n",
    "    # If results are paginated, perform additional GET queries and create new dataframes until no more pages are left\n",
    "    while cursor:\n",
    "        search_url3 = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results?cursor={cursor}\"\n",
    "        search_resp_2 = requests.get(url=search_url3, headers=headers).json()\n",
    "        for item in search_resp_2[\"data\"]:\n",
    "            datestr = item['start_time'].split(\"T\")[0]\n",
    "            date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n",
    "            year = date_obj.year\n",
    "            month = date_obj.month\n",
    "            isodate = date_obj.isocalendar()\n",
    "            if isodate[0] == (year - 1) and isodate[1] == 52:\n",
    "                week = 1\n",
    "            else:\n",
    "                week = isodate[1]\n",
    "\n",
    "            sat_type = get_sat_type(item['product_name'], item['source'])\n",
    "            item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n",
    "                                    \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n",
    "                                    item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n",
    "                                    \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n",
    "            df_list.append(item_df)\n",
    "        \n",
    "        try:\n",
    "            cursor = search_resp_2['pagination']['cursor']['next']\n",
    "        except:\n",
    "            cursor = None\n",
    "    \n",
    "    return df_list\n",
    "\n",
    "# This loop iterates through the shorter date ranges created in the previous cell,\n",
    "# creating new search queries and retrieving results. The returned json is then parsed\n",
    "# to create Pandas dataframes, which are captured in a list\n",
    "\n",
    "df_list = [] # Empty list used to hold the separate results dataframes\n",
    "for entry in date_list:\n",
    "    start_date = entry['start_date']\n",
    "    end_date = entry['end_date']\n",
    "    print(f\"running search query for date range: {start_date} to {end_date}\")\n",
    "    query = create_search_query(start_date, end_date, aoi_json)\n",
    "    search_id = search_catalog(query, headers)\n",
    "    search_resp = get_search_results(search_id, headers)\n",
    "    df_list = parse_results(search_resp, headers, search_id, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell combines the returned data from the separate API queries into a single dataframe\n",
    "def combine_query_dataframes(df_list):\n",
    "    \"\"\"\n",
    "    Combined separate results dataframes into a single dataframe.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df_list: list\n",
    "        list of separate dataframes created from query results\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    search_df: pandas dataframe\n",
    "        combined dataframe containing all search results\n",
    "    \"\"\"\n",
    "    search_df = pd.concat(df_list)\n",
    "    search_df.reset_index(drop=True, inplace=True)\n",
    "    search_df['year'] = search_df['year'].astype('category')\n",
    "    search_df['month'] = search_df['month'].astype('category')\n",
    "    search_df['week'] = search_df['week'].astype('category')\n",
    "\n",
    "    # this line adds a new field with an image preview html link, which is used in one of the interactive graphs in Stage 3\n",
    "    search_df[\"preview_html\"] = search_df.apply(lambda x: f'<a href=\\\"{x[\"preview\"]}\\\">Preview</a>', axis=1)\n",
    "\n",
    "    return search_df\n",
    "\n",
    "search_df = combine_query_dataframes(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put combined search dataframe into pickle file to reload later\n",
    "search_pickle = outpath_search.joinpath(f'{aoi_name}{o_start_date}_to_{o_end_date}_search_df.pkl')\n",
    "search_df.to_pickle(search_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Filter Query Dataframe.\n",
    "This stage of the process filters down the query results from Stage 1. Continue from here if you ran Stage 1 previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in query pickle file\n",
    "search_pickle = outpath_search.joinpath(f'{aoi_name}{o_start_date}_to_{o_end_date}_search_df.pkl')\n",
    "\n",
    "with open(search_pickle, \"rb\") as f:\n",
    "    search_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates(df):\n",
    "    \"\"\"\n",
    "    If more than one image exists for a given day, this function will select the \"best\" image for the day.\n",
    "    First, images with poor AOI coverage are removed. If multiple images remain after this step, then the\n",
    "    images are filtered by sensor type, with superdove and dove-r being prioritized over dove-classic.\n",
    "    Finally, if multiple images still remain, the image with the highest AOI coverage is selected.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        subset dataframe containing only the images for the specific date\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    select_id: str\n",
    "        image id string for the selected image\n",
    "    \"\"\"\n",
    "\n",
    "    top_coverage = df.nlargest(1,'aoi_coverage')[\"aoi_coverage\"].tolist()[0]\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        df['cover_percentmax'] = df.apply(lambda row: (row[\"aoi_coverage\"] / top_coverage), axis=1)\n",
    "        \n",
    "    df = df.loc[(df[\"cover_percentmax\"]>=0.9)]\n",
    "\n",
    "    if 'superdove' in df['sat_type'].unique() or 'dove-r' in df['sat_type']:\n",
    "        df = df.loc[(df['sat_type'] == 'superdove') | (df['sat_type'] == 'dove-r')]\n",
    "\n",
    "    df = df.nlargest(1,'aoi_coverage')\n",
    "    select_id = df['id'].values[0]\n",
    "\n",
    "    return select_id\n",
    "\n",
    "def filter_dataframe(df, cloud_cover_thresh, coverage_thresh):\n",
    "    \"\"\"\n",
    "    Filters down initial query results based on cloud cover, aoi coverage, and selecting only one\n",
    "    image per day.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        dataframe containing full image search results\n",
    "    cloud_cover_thresh: float\n",
    "        maximum cloud cover percent threshold\n",
    "    coverage_thresh: float\n",
    "        minimum aoi area coverage percent threshold\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    filtered_df: pandas dataframe\n",
    "        filtered image dataframe\n",
    "    \"\"\"\n",
    "    filtered_df = df.copy(deep=True)\n",
    "    filtered_df = filtered_df.loc[(search_df[\"cloud_cover\"]<=cloud_cover_thresh) & (filtered_df[\"aoi_coverage\"] >= coverage_thresh)]\n",
    "\n",
    "    id_list = []\n",
    "    unique_dates = filtered_df.datestr.unique().tolist()\n",
    "    for date_ in unique_dates:\n",
    "        filtered_df2 = filtered_df.loc[filtered_df[\"datestr\"]==date_]\n",
    "        if len(filtered_df2) > 1:\n",
    "            select_id = filter_dates(filtered_df2)\n",
    "        else:\n",
    "            select_id = filtered_df2['id'].values[0]\n",
    "        \n",
    "        id_list.append(select_id)\n",
    "\n",
    "    filtered_df = filtered_df[filtered_df['id'].isin(id_list)]\n",
    "\n",
    "    filtered_df.reset_index(inplace=True, drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "filtered_search = filter_dataframe(search_df, cc_thresh, coverage_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save filtered dataframe to pickle file for subsequent process stages\n",
    "filter_search_pickle = outpath_search.joinpath(f'{aoi_name}{o_start_date}_to_{o_end_date}_search_df_filtered.pkl')\n",
    "filtered_search.to_pickle(filter_search_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Graph Generation\n",
    "This stage is entirely optional but can be used to visualize the results of the query prior to download. The results of the query are displayed in a variety of interactive graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to put put figures\n",
    "outpath_figs = outpath_search.joinpath('figures')\n",
    "outpath_figs.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read filtered search dataframe from pickle file\n",
    "filter_search_pickle = outpath_search.joinpath(f'{aoi_name}{o_start_date}_to_{o_end_date}_search_df_filtered.pkl')\n",
    "with open(filter_search_pickle, \"rb\") as f:\n",
    "    filtered_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 1: monthly bar graphs showing the number of images per month broken down by cloud cover percentage\n",
    "def create_month_cc_graph(df, target_year, output_graph=False):\n",
    "    \"\"\"\n",
    "    Creates interactive bar plots showing the number of images in a given year, broken down by month\n",
    "    and by cloud cover percentage.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    target_year: int\n",
    "        year to produce graph for (will cause an error if no images present for specified year)\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    \n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "    min_date = target_df.iloc[0]['datestr']\n",
    "    max_date = target_df.iloc[-1]['datestr']\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    month_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['month'].value_counts()\n",
    "    month_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['month'].value_counts()\n",
    "    month_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['month'].value_counts()\n",
    "    month_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['month'].value_counts()\n",
    "    month_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['month'].value_counts()\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Month - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Monthly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_month_cc_graph(filtered_search, target_year=2022, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 2: Weekly bar graphs showing the number of images per month broken down by cloud cover percentage\n",
    "def create_week_cc_graph(df, target_year, output_graph=False):\n",
    "    \"\"\"\n",
    "    Creates interactive bar plots showing the number of images in a given year, broken down by the week of year\n",
    "    and by cloud cover percentage.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    target_year: int\n",
    "        year to produce graph for (will cause an error if no images present for specified year)\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "    min_date = target_df.iloc[0]['datestr']\n",
    "    max_date = target_df.iloc[-1]['datestr']\n",
    "    week_df = pd.DataFrame()\n",
    "    week_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['week'].value_counts()\n",
    "    week_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['week'].value_counts()\n",
    "    week_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['week'].value_counts()\n",
    "    week_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['week'].value_counts()\n",
    "    week_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['week'].value_counts()\n",
    "\n",
    "    week_df.reset_index(inplace=True)\n",
    "    week_df.rename(columns={'index': 'week'}, inplace=True)\n",
    "    week_df.sort_values('week', inplace=True)\n",
    "    week_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    week_fig = px.bar(week_df, x='week', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n",
    "    week_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Week - {min_date} to {max_date} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Week of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Weekly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        week_fig.write_image(out_png)\n",
    "        week_fig.write_html(out_html)\n",
    "\n",
    "    week_fig.show()\n",
    "\n",
    "create_week_cc_graph(filtered_search, target_year=2022, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 3: Annual bar chart showing image counts per year broken down by cloud cover percentage.\n",
    "# Use this to display the number of images across multiple years in the query, if applicable.\n",
    "def create_year_cc_graph(df, output_graph=False):\n",
    "    \"\"\"\n",
    "    Creates interactive bar plots showing the number of images in a given year across multiple years,\n",
    "    broken down by cloud cover percentage.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    target_df = df\n",
    "    min_year = target_df.iloc[0]['year']\n",
    "    max_year = target_df.iloc[-1]['year']\n",
    "    avg_price = target_df[\"cost\"].mean()\n",
    "    year_df = pd.DataFrame()\n",
    "    year_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['year'].value_counts()\n",
    "    year_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['year'].value_counts()\n",
    "    year_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['year'].value_counts()\n",
    "    year_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['year'].value_counts()\n",
    "    year_df[\">10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 10)]['year'].value_counts()\n",
    "    year_df[\"0% clouds cost\"] = year_df.apply(lambda row: round((row[\"0% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\"<=2% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=2% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\"<=5% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=5% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\"<=10% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=10% clouds\"] * avg_price), 2), axis=1)\n",
    "    year_df[\">10% clouds cost\"] = year_df.apply(lambda row: round((row[\">10% clouds\"] * avg_price), 2), axis=1)\n",
    "\n",
    "    year_df.reset_index(inplace=True)\n",
    "    year_df.rename(columns={'index': 'year'}, inplace=True)\n",
    "    year_df.sort_values('year', inplace=True)\n",
    "    year_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    h_template='''Image Count 0% Clouds: %{customdata[0]} Estimated Cost: %{customdata[5]}<br>\n",
    "                Image Count <=2% Clouds: %{customdata[1]} Estimated Cost: %{customdata[6]}<br>\n",
    "                Image Count <=5% Clouds: %{customdata[2]} Estimated Cost: %{customdata[7]}<br>\n",
    "                Image Count <=10% Clouds: %{customdata[3]} Estimated Cost: %{customdata[8]}<br>\n",
    "                Image Count >10% Clouds: %{customdata[4]} Estimated Cost: %{customdata[9]}'''\n",
    "    year_fig = px.bar(year_df, x='year', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], \n",
    "                      color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"}, \n",
    "                      custom_data=np.stack((year_df['0% clouds'], year_df['<=2% clouds'], year_df['<=5% clouds'], year_df['<=10% clouds'], year_df['>10% clouds'], \n",
    "                                            year_df['0% clouds cost'], year_df['<=2% clouds cost'], year_df['<=5% clouds cost'], year_df['<=10% clouds cost'], year_df['>10% clouds cost'])))\n",
    "    \n",
    "    year_fig.update_layout(title_text=f\"PlanetScope Annual Image Counts and Cost - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n",
    "    year_fig.update_traces(hovertemplate=h_template)\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Annual_Image_Count_and_Cost_{min_year}_to_{max_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        year_fig.write_image(out_png)\n",
    "        year_fig.write_html(out_html)\n",
    "\n",
    "    year_fig.show()\n",
    "\n",
    "    return year_df\n",
    "\n",
    "year_df = create_year_cc_graph(filtered_search, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 4: Monthly bar chart showing monthly image counts broken down by year. User must specify a threshold for cloud cover\n",
    "def create_month_multiyear_graph(df, cc_threshold, output_graph=False):\n",
    "    \"\"\"\n",
    "    Creates a monthly bar chart to display image counts per month, broken down by imaging year.\n",
    "    Image counts are determined based on the input cloud cover threshold.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    cc_threshold: float\n",
    "        cloud cover percentage threshold. Images with cloud cover exceeding this number will not be counted\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    \n",
    "    min_year = df.iloc[0]['year']\n",
    "    max_year = df.iloc[-1]['year']\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    col_list = []\n",
    "    for year in df.year.unique().tolist():\n",
    "        target_df = df.loc[df[\"year\"] == year]\n",
    "        col_name = f\"{year}_counts\"\n",
    "        month_df[col_name] = target_df.loc[target_df['cloud_cover'] <= cc_threshold]['month'].value_counts()\n",
    "        col_list.append(col_name)\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    month_fig = px.bar(month_df, x='month', y=col_list, barmode='group')\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and Month - {cc_threshold}% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Image_Count_Annual_Monthly_{cc_threshold}%CC_{min_year}_to_{max_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_month_multiyear_graph(filtered_search, cc_threshold=0, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 5: Scatter plot showing individual images, with date on the x-axis and cloud cover percentage\n",
    "# on the y-axis. Hovering over a point displays image information, including a preview link to display the image thumbnail\n",
    "def create_image_preview_graph(df, output_graph=False):\n",
    "    \"\"\"\n",
    "    Creates an interactive scatterplot displaying individual images, with the image date on the \n",
    "    x-axis and cloud cover percentage on the y-axis.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    layout = go.Layout(title=f'PlanetScope Images - {o_start_date} to {o_end_date} - {aoi_name}', xaxis=dict(title=\"Image Date\"), yaxis=dict(title=\"Cloud Cover Percentage\"))#, autosize=False, width=1200, height=510)\n",
    "\n",
    "    fig = go.Figure(layout=layout)\n",
    "\n",
    "    h_template='Image Date: %{customdata[0]}<br>%{customdata[3]}<br>Cloud Cover: %{customdata[2]}%<br>Image Source: %{customdata[1]}'\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=df['cloud_cover'],\n",
    "        x=df['date'],\n",
    "        mode='markers',\n",
    "        orientation='h',\n",
    "        marker=dict(\n",
    "            color='red'),\n",
    "        customdata=np.stack((df['datestr'], df['source'], df['cloud_cover'], df['preview_html']), axis=-1),\n",
    "        hovertemplate=h_template\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title={'x': 0.5, 'xanchor': 'center'})\n",
    "\n",
    "    filenamestr = outpath_figs.joinpath(f\"PlanetScope_Image_Previews_{o_start_date}_to_{o_end_date}_{aoi_name}\")\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        fig.write_image(out_png)\n",
    "        fig.write_html(out_html)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "create_image_preview_graph(filtered_search, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 6: Monthly bar chart showing monthly image counts for a given year broken down by satellite platform (dove-classic, dove-r, and superdove). \n",
    "# User must specify a threshold for cloud cover\n",
    "def create_dove_monthly_graph(df, target_year, cc_threshold, output_graph):\n",
    "    \"\"\"\n",
    "    Creates a monthly bar chart to display image counts per month for a specified year, broken down by satellite platform\n",
    "    Image counts are determined based on the input cloud cover threshold.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    target_year: int\n",
    "        year to produce graph for (will cause an error if no images present for specified year)\n",
    "    cc_threshold: float\n",
    "        cloud cover percentage threshold. Images with cloud cover exceeding this number will not be counted\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    \n",
    "    target_df = df.loc[df[\"year\"] == target_year]\n",
    "\n",
    "    month_df = pd.DataFrame()\n",
    "    col_list = []\n",
    "    for datasource in df.sat_type.unique().tolist():\n",
    "        if datasource == \"dove-c\":\n",
    "            col_name = \"Dove-Classic\"\n",
    "        elif datasource == \"dove-r\":\n",
    "            col_name = \"Dove-R\"\n",
    "        elif datasource == \"superdove\":\n",
    "            col_name = \"SuperDove\"\n",
    "        \n",
    "        month_df[col_name] = target_df.loc[(target_df['cloud_cover'] == cc_threshold) & (target_df['sat_type'] == datasource)]['month'].value_counts()\n",
    "        col_list.append(col_name)\n",
    "\n",
    "    month_df.reset_index(inplace=True)\n",
    "    month_df.rename(columns={'index': 'month'}, inplace=True)\n",
    "    month_df.sort_values('month', inplace=True)\n",
    "    month_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    month_fig = px.bar(month_df, x='month', y=col_list)\n",
    "    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Month and By Satellite Type - {cc_threshold}% Cloud Cover - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Image_Count_Monthly_{cc_threshold}%CC_SatelliteType_{target_year}_{aoi_name}\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        month_fig.write_image(out_png)\n",
    "        month_fig.write_html(out_html)\n",
    "\n",
    "    month_fig.show()\n",
    "\n",
    "create_dove_monthly_graph(filtered_search, target_year=2021, cc_threshold=0, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 7: Annual bar chart showing image counts per year broken down by satellite platform.\n",
    "# Use this to display the number of images from different satellites across multiple years in the query, if applicable.\n",
    "def create_dove_year_graph(df, cc_threshold, output_graph):\n",
    "    \"\"\"\n",
    "    Creates a yearly bar chart to display image counts per year over multiple years, broken down by satellite platform\n",
    "    Image counts are determined based on the input cloud cover threshold.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    df: pandas dataframe\n",
    "        query dataframe containing image information\n",
    "    cc_threshold: float\n",
    "        cloud cover percentage threshold. Images with cloud cover exceeding this number will not be counted\n",
    "    output_graph: bool\n",
    "        if set to True, graph will be saved to file as an html and png file\n",
    "    \"\"\"\n",
    "    target_df = df\n",
    "    min_year = df.iloc[0]['year']\n",
    "    max_year = df.iloc[-1]['year']\n",
    "  \n",
    "    year_df = pd.DataFrame()\n",
    "    year_df[\"Dove-Classic\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-c\")]['year'].value_counts()\n",
    "    year_df[\"Dove-R\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-r\")]['year'].value_counts()\n",
    "    year_df[\"SuperDove\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"superdove\")]['year'].value_counts()\n",
    "    \n",
    "    year_df.reset_index(inplace=True)\n",
    "    year_df.rename(columns={'index': 'year'}, inplace=True)\n",
    "    year_df.sort_values('year', inplace=True)\n",
    "    year_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    year_fig = px.bar(year_df, x='year', y=[\"Dove-Classic\", \"Dove-R\", \"SuperDove\"])\n",
    "    year_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and By Satellite Type - {cc_threshold}% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n",
    "\n",
    "    filenamestr = f\"PlanetScope_Annual_Image_Count_{cc_threshold}_%CC_SatelliteType_{min_year}_to_{max_year}_{aoi_name}_filtered\"\n",
    "    if output_graph:\n",
    "        out_png = outpath_figs.joinpath(f\"{filenamestr}.png\")\n",
    "        out_html = outpath_figs.joinpath(f\"{filenamestr}.html\")\n",
    "        year_fig.write_image(out_png)\n",
    "        year_fig.write_html(out_html)\n",
    "\n",
    "    year_fig.show()\n",
    "\n",
    "    return year_df\n",
    "\n",
    "year_df = create_dove_year_graph(filtered_search, cc_threshold=0, output_graph=output_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Download Imagery\n",
    "In this stage, the images returned by the query in Stage 1 are downloaded. The user may input final criteria to further subset the images to retrieve based on cloud cover, start and end dates, AOI coverage, and satellite type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read filtered search dataframe from pickle file\n",
    "filter_search_pickle = outpath_search.joinpath(f'{aoi_name}_{o_start_date}_to_{o_end_date}_search_df_filtered.pkl')\n",
    "with open(filter_search_pickle, \"rb\") as f:\n",
    "    filtered_search = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify final image download selection criteria here. The original query criteria can also be used by setting all variables to None\n",
    "max_cc_download = None # Maximum cloud cover percentage to use for image download\n",
    "start_date_download = None # Start date for image download (string in the format \"YYYY-MM-DD\")\n",
    "end_date_download = None # End date for image download (string in the format \"YYYY-MM-DD\")\n",
    "sat_list = None # List of satellite types to download. Options are 'dove-c', 'dove-r', and 'superdove'. Note: TDI notebook currently only supports superdove\n",
    "aoi_coverage_download = None # Minimum AOI coverage percentage to use for image download\n",
    "\n",
    "# Examples of user-specified criteria for image download\n",
    "# max_cc_download = 0.0\n",
    "# start_date_download = \"2023-01-01\"\n",
    "# end_date_download = \"2023-03-31\"\n",
    "# sat_list = ['superdove']\n",
    "# aoi_coverage_download = 95.0\n",
    "\n",
    "# If one of the above download criteria is not specified, the original query parameter is used.\n",
    "if max_cc_download is None:\n",
    "    max_cc_download = cc_thresh\n",
    "if aoi_coverage_download is None:\n",
    "    aoi_coverage_download = coverage_thresh\n",
    "if start_date_download is None:\n",
    "    start_date_download = o_start_date\n",
    "if end_date_download is None:\n",
    "    end_date_download = o_end_date\n",
    "if sat_list is None:\n",
    "    sat_list = ['dove-c', 'dove-r', 'superdove']\n",
    "    \n",
    "\n",
    "start_date_final = datetime.datetime.strptime(start_date_download, \"%Y-%m-%d\")\n",
    "end_date_final = datetime.datetime.strptime(end_date_download, \"%Y-%m-%d\")\n",
    "\n",
    "# Path to put downloaded images\n",
    "outpath_dl = out_path.joinpath(out_path, \"images\", f\"{aoi_name}_{start_date_download}_to_{end_date_download}\")\n",
    "outpath_dl.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to put download dataframes\n",
    "outpath_download_df = out_path.joinpath('download_dataframes')\n",
    "outpath_download_df.mkdir(exist_ok=True)\n",
    "\n",
    "filtered_download_pickle = outpath_download_df.joinpath(f'{aoi_name}_{start_date_download}_to_{end_date_download}_download_df.pkl')\n",
    "filtered_download_resp_pickle = outpath_download_df.joinpath(f'{aoi_name}_{start_date_download}_to_{end_date_download}_download_resp_dict.pkl')\n",
    "\n",
    "pl_resp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Filter results down to final download list based on above criteria\n",
    "filtered_search_final = filtered_search.copy(deep=True)\n",
    "filtered_search_final = filtered_search_final.loc[(filtered_search_final['date'] >= start_date_final) \n",
    "                                                  & (filtered_search_final['date'] <= end_date_final) \n",
    "                                                  & (filtered_search_final['cloud_cover'] <= max_cc_download)\n",
    "                                                  & (filtered_search_final['aoi_coverage'] >= aoi_coverage_download)\n",
    "                                                  & (filtered_search_final['sat_type'].isin(sat_list))]\n",
    "\n",
    "filtered_search_final['pl_id'] = \"\"\n",
    "filtered_search_final['pl_status'] = \"\"\n",
    "\n",
    "filtered_search_final.to_pickle(filtered_download_pickle)\n",
    "\n",
    "total_cost = round(filtered_search_final['cost'].sum(), 2)\n",
    "num_images = len(filtered_search_final)\n",
    "print(f\"Total number of images to download: {num_images}. Total cost: ${total_cost} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query endpoint to get different output types and build dictionary of selectable output types for download\n",
    "output_id_url = f\"https://api.skywatch.co/earthcache/outputs\"\n",
    "output_id_resp = requests.get(url=output_id_url, headers=headers).json()\n",
    "output_type_dict = {}\n",
    "for output in output_id_resp['data']:\n",
    "    output_name = output['name']\n",
    "    output_id = output['id']\n",
    "    output_type_dict[output_name] = output_id\n",
    "\n",
    "output_type_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set desired output type to download based on the output from the previous cell\n",
    "# Note: leave as \"All Optical Bands\" if using the TDI notebook after image download\n",
    "im_download_product = 'All Optical Bands'\n",
    "output_id = output_type_dict[im_download_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_query(pipeline_name, search_id, image_id, output_id):\n",
    "    \"\"\"\n",
    "    Creates a json query to use for creating an image download pipeline in the Skywatch API.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    pipeline_name: str\n",
    "        Name of the image download pipeline\n",
    "    search_id: str\n",
    "        id for search query where image appears\n",
    "    image_id: str\n",
    "        id for image\n",
    "    output_id: str\n",
    "        id string for selected image product to download\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    query: dict\n",
    "        formatted query dictionary\n",
    "    \"\"\"\n",
    "    query = {\n",
    "            \"name\": pipeline_name,\n",
    "            \"search_id\": search_id,\n",
    "            \"search_results\": image_id,\n",
    "            \"output\": {\n",
    "                \"id\": output_id,\n",
    "                \"format\": \"geotiff\",\n",
    "                \"mosaic\": \"off\"\n",
    "            },\n",
    "            }\n",
    "    \n",
    "    return query\n",
    "\n",
    "def post_pipeline(query, headers):\n",
    "    \"\"\"\n",
    "    Runs a POST request to create image download pipeline.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    query: dict\n",
    "        formatted query dictionary\n",
    "    headers: dict\n",
    "        headers for query\n",
    "        \n",
    "    Returns:\n",
    "    ---------------------\n",
    "    pipeline_id: str\n",
    "        id string for download pipeline\n",
    "\n",
    "    \"\"\"\n",
    "    pipeline_url = \"https://api.skywatch.co/earthcache/pipelines\"\n",
    "    pipeline_resp = requests.post(url=pipeline_url, data=json.dumps(query), headers=headers).json()\n",
    "    pipeline_id = pipeline_resp['data']['id']\n",
    "    \n",
    "    return pipeline_id\n",
    "\n",
    "def get_pipeline(pipeline_id):\n",
    "    \"\"\"\n",
    "    Get request to get the status of an image download pipeline.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    pipeline_id: str\n",
    "        id string for download pipeline\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    pipeline_get_resp: requests response object\n",
    "        image download pipeline response object\n",
    "\n",
    "    \"\"\"\n",
    "    pipeline_get_url = f\"https://api.skywatch.co/earthcache/interval_results?pipeline_id={pipeline_id}\"\n",
    "    pipeline_get_resp = requests.get(url=pipeline_get_url, headers=headers)#.json()\n",
    "\n",
    "    return pipeline_get_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through all images in image download dataframe and creates new image download pipelines for each.\n",
    "for index, row in filtered_search_final.iterrows():\n",
    "    time.sleep(1)\n",
    "    search_id = row[\"search_id\"]\n",
    "    image_id = row[\"id\"]\n",
    "    product_name = row[\"product_name\"]\n",
    "    print(f\"creating download pipeline for image: {product_name}\")\n",
    "    pipeline_name = f\"{aoi_name}_{product_name}\" #Pipeline name, which shows up in the Earthcache console. Default combines the AOI name and the image name\n",
    "    pl_query = create_pipeline_query(pipeline_name, search_id, image_id, output_id)\n",
    "    pl_id = post_pipeline(pl_query, headers)\n",
    "    filtered_search_final.loc[index, 'pl_id'] = pl_id # Appends the image download pipeline ID to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the image download dataframe has been updated with the download pipeline IDs, \n",
    "# we save it to a pickle file in case notebook terminates before images are downloaded\n",
    "filtered_search_final.to_pickle(filtered_download_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve filtered download dataframe and dictionary containing json response objects in case\n",
    "# notebook was terminated before images could be downloaded. Resume from here in that case.\n",
    "\n",
    "if filtered_download_pickle.exists():\n",
    "    with open(filtered_download_pickle, 'rb') as f:\n",
    "        filtered_search_final = pickle.load(f)\n",
    "if filtered_download_resp_pickle.exists():\n",
    "    with open(filtered_download_resp_pickle, 'rb') as f:\n",
    "        pl_resp_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pipeline(pl_id):\n",
    "    \"\"\"\n",
    "    Queries the image download pipeline to check processing status and retrieve pipeline json\n",
    "    response containing image download links for completed pipelines.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    pl_id: str\n",
    "        id string for download pipeline\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "    pl_status: str\n",
    "        processing status of pipeline\n",
    "    pl_resp_json: dict\n",
    "        json response object from pipeline query. For completed jobs this will contain\n",
    "        links for image download.\n",
    "    \"\"\"\n",
    "    pl_resp = get_pipeline(pl_id)\n",
    "    pl_resp_json = pl_resp.json()\n",
    "    pl_status = pl_resp_json['data'][0]['status']\n",
    "    return pl_status, pl_resp_json\n",
    "\n",
    "# Iterates through all images in image download dataframe and checks the status of the image \n",
    "# download pipelines and saves the response json to a dictionary.\n",
    "# If any image download pipelines are still processing, the loop will repeat after a delay \n",
    "# which gets progressively shorter. The initial delay is set to 30 minutes. The delay is progressively \n",
    "# reduced, with a minimum delay of 1 minute being imposed. The loop repeats until the status of \n",
    "# all download pipelines have changed to completed.\n",
    "\n",
    "num_iterations = 1\n",
    "results_processing = True\n",
    "while results_processing:\n",
    "    status_list = []\n",
    "    print('Checking status of image download pipelines.')\n",
    "    for index, row in filtered_search_final.iterrows():\n",
    "        time.sleep(1)\n",
    "        pl_status = row['pl_status']\n",
    "        pl_id = row['pl_id']\n",
    "        if pl_status != 'complete':\n",
    "            pl_status, pl_resp_json = query_pipeline(pl_id)\n",
    "            filtered_search_final.loc[index, 'pl_status'] = pl_resp_json['data'][0]['status']\n",
    "            pl_resp_dict[pl_id] = pl_resp_json\n",
    "        \n",
    "        status_list.append(pl_status)\n",
    "\n",
    "    filtered_search_final.to_pickle(filtered_download_pickle)\n",
    "    with open(filtered_download_resp_pickle, 'wb') as f:\n",
    "        pickle.dump(pl_resp_dict, f)\n",
    "    \n",
    "    if 'complete' in set(status_list) and len(set(status_list)) == 1:\n",
    "        results_processing = False\n",
    "        print('All image pipelines are finished and ready for download!')\n",
    "    else:\n",
    "        wait_time = 1800 / num_iterations\n",
    "        if wait_time < 60:\n",
    "            wait_time = 60\n",
    "        wait_time_mins = round((wait_time / 60), 0)\n",
    "        print(f\"Results still pending for some items. Waiting for {wait_time_mins} mins and trying again.\")\n",
    "        num_iterations += 2\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, out_name):\n",
    "    \"\"\"\n",
    "    Downloads files using the provided URL.\n",
    "\n",
    "    Params:\n",
    "    ---------------------\n",
    "    url: str\n",
    "        url path to the file\n",
    "    out_name: str\n",
    "        output file name for the local file download\n",
    "\n",
    "    Returns:\n",
    "    ---------------------\n",
    "\n",
    "    \"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(out_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=None):\n",
    "                f.write(chunk)\n",
    "\n",
    "# Iterates through all images in image download dataframe and downloads image and metadata\n",
    "for index, row in filtered_search_final.iterrows():\n",
    "    product_name = row['product_name']\n",
    "    pl_id = row['pl_id']\n",
    "    pl_resp_json = pl_resp_dict[pl_id]\n",
    "    results = pl_resp_json['data'][0]['results'][0]\n",
    "\n",
    "    # Dict of image files to download. Currently downloads image tif file and metadata json file.\n",
    "    # Cloud mask file currently commented out, as it is not used in subsequent processes.\n",
    "    download_dict = {'image': {'url': results['analytics_url'], 'extension': 'analytic.tif'},\n",
    "                    'metadata': {'url': results['metadata_url'], 'extension': 'metadata.json'}}\n",
    "                    # 'cloud_mask': {'url': results['raster_files'][0]['uri'], 'extension': 'mask.tif'}}\n",
    "\n",
    "    out_basepath = outpath_dl.joinpath(product_name) # Files for each image are placed in a folder named after the image file\n",
    "    out_basepath.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f'downloading all files for image: {product_name}')\n",
    "    for key, value in download_dict.items():\n",
    "        dl_url = value['url']\n",
    "        extension = value['extension']\n",
    "        out_fname = out_basepath.joinpath(f'{product_name}_{extension}')\n",
    "        download_file(dl_url, out_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b205056744192b51d7fcf647b8488be922fc7e73105947b81a7c7b6d880083f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
