{
  "hash": "4035d0c08138f59b5b0274c7b1c6b29b",
  "result": {
    "markdown": "---\ntitle: \"Beitbridge Time Series Modelling\"\ntoc: true\nformat:\n  html:\n    html-math-method: katex\n    code-tools: true\nexecute:\n  warning: false\n---\n\n\nThe following notebook is not complete.\\\n\\\nThe intention of this notebook is to attempt to model border wait time based on the border wait time data provided by World Bank. In the future, this was intended to be correlated and/or modelling with remote sensing derived traffic density index (TDI).\n\n------------------------------------------------------------------------\n\nTo approach this problem, we can use a time-series analysis technique. Since we are given data over three years, we can split it into training and testing datasets to evaluate the model's performance. Here are the steps I would take:\n\n1\\. **Data exploration and cleaning:** Explore the dataset to check for any missing or inconsistent data. Also, visualize the data to understand its patterns and trends. This was done in our `gps_eda.qmd` notebook.\n\n2\\. **Feature engineering:** Create new features that could potentially help in predicting the wait time, such as day of the week, month, holidays, and any significant events that could impact border crossing.\n\n3\\. **Time-series modeling:** Since our data is time-series, we can use a time-series model to predict wait times. We can use various models such as Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA (SARIMA), or Prophet.\n\n4\\. **Model validation:** After training the model, we can use the test dataset to evaluate its performance. We can use metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) to evaluate the model's accuracy.\n\n5\\. **Visualization:** Finally, we can visualize the predicted wait times against the actual wait times to understand the model's performance visually.\n\n## Caveats and uncertainties:\n\n1\\. **Corrupt area**: Since there has been reports of corruption, we can not guarantee the authenticity of the data. There could be bribes to cross the border quicker than others.\n\n2\\. **External factors**: There could be external factors such as political instability, social unrest, or natural disasters, which could impact the border wait times. Since our dataset does not include these factors, our model might not perform well in predicting such events.\n\n3\\. **Limited dataset**: We only have three years of data, which might not be enough to capture long-term trends and patterns accurately. If there are significant changes in the border crossing policies or infrastructure, our model might not be able to capture such changes.\n\nOverall, while a time-series model could help predict border wait times, we need to be aware of the caveats and uncertainties that could impact the model's accuracy.\n\n## Read in the libraries and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(caret)\nlibrary(mlbench)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nborder_data = read_csv(\"../data/processed/Beitbridge_Counts_Wait_Time_2018_2022.csv\")\n```\n:::\n\n\n# Subsets\n\n## SA - Zimbabwe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsa_zimbabwe = border_data %>% filter(Direction == \"SA-Zimbabwe\")\n```\n:::\n\n\n## Zimbabwe - SA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzimbabwe_sa = border_data %>% filter(Direction == \"Zimbabwe-SA\")\n```\n:::\n\n\n## Plotting\n\n### Convert to time series\n\n\n::: {.cell}\n\n```{.r .cell-code}\nborder_data_ts <- sa_zimbabwe \nborder_data_ts$datetime <- ymd_h(str_c(border_data_ts $StartDate, border_data_ts $StartHour))\nborder_data_ts = border_data_ts%>% select(datetime, Median_Minutes, Count_Events) %>% \n  as_tsibble(index = datetime)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_counts <- sa_zimbabwe%>%\n  group_by(date=date(StartDate))%>%\n  summarize(Daily_Counts = sum(Count_Events,na.rm=TRUE))%>%\n  ungroup()%>%\n  as_tsibble(index=date)\n```\n:::\n\n\n### Moving averages\n\nCreating moving averages for 5 days, 7 days and 14 days.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_counts = daily_counts %>% mutate(ma_5 = ma(Daily_Counts, 5), ma_7=ma(Daily_Counts,7), ma_14 = ma(Daily_Counts, 14))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_counts %>%\n  #filter(year(date)==2021)%>%\n  ggplot()+\n  geom_line(aes(x=date, y=Daily_Counts, col=\"Daily Counts\"))+\n  geom_line(aes(x=date, y=ma_14, col=\"14-ma\"))+\n  scale_colour_manual(\"Legend\",values = c(\"14-ma\" = \"blue\", \"Daily Counts\"=\"grey\")) +\n  labs(y = \"Daily Counts\", x = \"Date\")\n```\n\n::: {.cell-output-display}\n![](gps_modelling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nZoom in on year 2021 \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_counts %>%\n  filter(year(date)==2021)%>%\n  ggplot()+\n  geom_line(aes(x=date, y=Daily_Counts, col=\"Daily Counts\"))+\n  geom_line(aes(x=date, y=ma_14, col=\"14-ma\"))+\n  scale_colour_manual(\"Legend\",values = c(\"14-ma\" = \"blue\", \"Daily Counts\"=\"grey\")) +\n  labs(y = \"Daily Counts\", x = \"Date\")\n```\n\n::: {.cell-output-display}\n![](gps_modelling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Seasonal Decomposition\n\n- did not get to implement or test further due to budgetary constraints \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(daily_counts)\n```\n\n::: {.cell-output-display}\n![](gps_modelling_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Caret - Linear Model\n\n-   We can create additional features from the GPS data like:\n\n    -   Day of week\n\n    -   Month\n\n    -   Year\n\n    -   Hour\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the necessary libraries\n\n\n# Read in the data\nborder_data <- zimbabwe_sa\n#border_data<-read_csv('data.csv')\n# Convert date and hour columns to datetime format\nborder_data$datetime <- ymd_h(paste(border_data$StartDate, border_data$StartHour))\n\n# Create new features\nborder_data$day_of_week <- weekdays(border_data$datetime)\nborder_data$hour_of_day <- hour(border_data$datetime)\nborder_data$month <- month(border_data$datetime)\nborder_data$year <- year(border_data$datetime)\n# Split the data into training and testing sets\nset.seed(123)\nborder_data = border_data %>% filter(!is.na(Median_Minutes))\ntraining_indices <- createDataPartition(border_data$Median_Minutes, p = 0.8, list = FALSE)\nborder_data_train <- border_data[training_indices,]\nborder_data_test <- border_data[-training_indices,]\n\n# Define the model formula\nmodel_formula <- Median_Minutes~ day_of_week + hour_of_day + month + year + Count_Events\n\n# Train the model using caret\nborder_model <- train(model_formula, data = border_data_train, method = \"lm\")\n\n# Make predictions on the testing set\nborder_predictions <- predict(border_model, newdata = border_data_test)\n\n# Evaluate the model's performance using Mean Absolute Error\nmae <- mean(abs(border_predictions - border_data_test$Median_Minutes))\n\n# Print the MAE\nprint(mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 678.9349\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(border_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1494   -605   -294     63  41854 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          -5.088e+05  1.504e+04 -33.828  < 2e-16 ***\nday_of_weekMonday    -2.519e+01  3.976e+01  -0.633   0.5264    \nday_of_weekSaturday   4.554e+01  3.920e+01   1.162   0.2454    \nday_of_weekSunday    -2.740e+01  3.919e+01  -0.699   0.4844    \nday_of_weekThursday   1.736e+01  3.905e+01   0.445   0.6566    \nday_of_weekTuesday   -6.136e+01  3.914e+01  -1.568   0.1170    \nday_of_weekWednesday -7.014e+00  3.913e+01  -0.179   0.8577    \nhour_of_day           1.600e+01  1.846e+00   8.670  < 2e-16 ***\nmonth                -7.094e+00  3.093e+00  -2.294   0.0218 *  \nyear                  2.523e+02  7.447e+00  33.874  < 2e-16 ***\nCount_Events          2.305e+01  3.662e+00   6.294 3.16e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1528 on 21174 degrees of freedom\nMultiple R-squared:  0.05729,\tAdjusted R-squared:  0.05684 \nF-statistic: 128.7 on 10 and 21174 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Random Forests\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the dataset\ndf = border_data\ndf = df %>% select(Count_Events, Median_Minutes,datetime,day_of_week,year)\n```\n:::\n\n\n::: callout-important\nThis will take awhile to run on a laptop! Beware of running below:\n:::\n\n\n::: {.cell hash='gps_modelling_cache/html/random-forests_8f3823e4e78ac80491fb2c9b495d9413'}\n\n```{.r .cell-code}\n# Create a train/test split\nset.seed(123)\ntrainIndex <- createDataPartition(df$Median_Minutes, p = .8, list = FALSE)\ntrain <- df[ trainIndex,]\ntest <- df[-trainIndex,]\n\n# Fit a random forest model\nset.seed(123)\nrf_model <- train(Median_Minutes ~ ., data = train, method = \"rf\")\n\n# Print the model results\nrf_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest \n\n21185 samples\n    4 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 21185, 21185, 21185, 21185, 21185, 21185, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared    MAE     \n  2     1487.270  0.09307127  697.2415\n  5     1546.652  0.06958624  713.6039\n  9     1677.811  0.05062974  758.2495\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n```\n:::\n:::\n\n\nIt is important to note, however, that the performance of a random forest model for predicting median border wait time would depend on the quality and relevance of the data and features used to train the model. Additionally, it may be necessary to regularly retrain the model with new data to ensure that it remains accurate over time.\n\n\nTODO: - subset by years, model individual years to account for covid - use moving averages - interpolate missing median wait time values - try different ARIMA models, GAMs and Prophet library\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}