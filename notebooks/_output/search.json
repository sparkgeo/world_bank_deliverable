[
  {
    "objectID": "tdi/tdi-notebook.html",
    "href": "tdi/tdi-notebook.html",
    "title": "Sparkgeo - World Bank Demo",
    "section": "",
    "text": "import os\nimport warnings\nfrom pathlib import Path\nimport subprocess\nimport rasterio as rio\nimport pandas as pd\nimport geopandas as gpd\nimport pickle\nimport xarray as xr\nimport rioxarray as rioxr\nimport numpy as np\nimport plotly.express as px\nfrom skimage import morphology\nfrom skimage.measure import label\nfrom skimage.metrics import structural_similarity as ssim\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures"
  },
  {
    "objectID": "tdi/tdi-notebook.html#set-up-paths-and-read-in-files-for-subsequent-processing",
    "href": "tdi/tdi-notebook.html#set-up-paths-and-read-in-files-for-subsequent-processing",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Set up paths and read in files for subsequent processing",
    "text": "Set up paths and read in files for subsequent processing\n\n#Main input filepath. All other files and folders should be nested under this one (aside from the road geom file specified below)\ninpath = Path(\"../data/processed/outputs_BeitBridge_2022-01-01_to_2023-03-27\")\n\n#Geometry file for clipping tdi outputs\nroad_geom_file = \"../data/processed/beitbridge_road_mask_v2.gpkg\"\n\nimg_path = inpath.joinpath(\"Beitbridge_PlanetScope\") #Subfolder containing images downloaded from skywatch api\n\noutpath_aligned = inpath.joinpath(\"Processed_Images\") #Output path for co-aligned images\noutpath_aligned.mkdir(exist_ok=True)\n\noutpath_workdir = inpath.joinpath(\"TDI_Outputs\") #Output directory for TDI file outputs\noutpath_workdir.mkdir(exist_ok=True)\n\nimg_df_csv = inpath.joinpath(\"search_df_2022-01-01_to_2023-03-27_filtered.csv\") #CSV file generated from dataframe created in the skywatch api notebook\nimg_df = pd.read_csv(img_df_csv)\nimg_df.drop(columns=['Unnamed: 0'], inplace=True)"
  },
  {
    "objectID": "tdi/tdi-notebook.html#stage-1-align-images",
    "href": "tdi/tdi-notebook.html#stage-1-align-images",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Stage 1: Align images",
    "text": "Stage 1: Align images\nThis section of the notebook is to take the PlanetScope images downloaded using the skywatch api notebook and resample them to a common pixel grid. Note: this step should only need to be run once.\n\n#Set up a base image to resample everything to match. It selects the first image with 100% AOI coverage to use as the base image\nbase_imgname = img_df.loc[img_df[\"aoi_coverage\"] == 100].iloc[0][\"product_name\"]\nbase_img_file = img_path.joinpath(base_imgname, f\"{base_imgname}_analytic.tif\")\nbase_img = rio.open(base_img_file)\n\n\ndef resample_image(base_img, warp_imgname, out_name):\n    \"\"\"\n    Function to resample an image to match the base image\n\n    Params:\n    ---------------------\n    base_img: rasterio dataset\n        base image to be aligned to\n    warp_imgname: pathlib Path\n        path to the image to be resampled.\n    out_name: pathlib Path\n        output path for resampled image\n    \"\"\"\n    \n    srs = base_img.crs.to_string()\n    pixelsize_x, pixelsize_y = base_img.res\n    bbox = base_img.bounds\n\n    #First step: create temp file using gdalwarp which resamples the target image to match the base image\n    cmd = f\"gdalwarp -t_srs {srs} -tap -tr {pixelsize_x} {pixelsize_y} -r near -te {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]} -te_srs {srs} -of vrt {warp_imgname} {out_name}.vrt -overwrite\"\n    subprocess.check_output(cmd, shell=True)\n\n    #Second step: use gdal_translate to compress the temp file into the final output\n    cmd2 = f\"gdal_translate -co compress=LZW {out_name}.vrt {out_name}.tif\"\n    subprocess.check_output(cmd2, shell=True)\n\n#Iterate through all the images in the img_df dataframe and resample to match the base image\nfor index, row in img_df.iterrows():\n    img_name = row[\"product_name\"]    \n    img_file = img_path.joinpath(img_name, f\"{img_name}_analytic.tif\")\n    img_processedname = outpath_aligned.joinpath(f\"{img_name}_aligned\")\n    resample_image(base_img, img_file, img_processedname)\n    \n    os.remove(f\"{img_processedname}.vrt\") #Delete temp file\n    print(f\"finished resampling image: {img_processedname}\")"
  },
  {
    "objectID": "tdi/tdi-notebook.html#stage-2-compute-median-image",
    "href": "tdi/tdi-notebook.html#stage-2-compute-median-image",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Stage 2: Compute median image",
    "text": "Stage 2: Compute median image\nThis section of the notebook is used to create a median image from all the aligned images. Note: this section should only need to be run once.\n\n#Iterate through all co-registered images and stack them into a big xarray\ndatasets = []\ndate_list = []\nfor index, row in img_df.iterrows():\n    img_name = row[\"product_name\"]\n    img_date = row['datestr']\n    img_file = outpath_aligned.joinpath(f\"{img_name}_aligned.tif\")\n    img_arr = rioxr.open_rasterio(img_file)\n    datasets.append(img_arr)\n    date_list.append(img_date)\n\ntime_var = xr.Variable('time', date_list)\ncombined_arr = xr.concat(datasets, dim=time_var)\n\n\n#Calculate the pixel and band-wise median from the stacked xarray\nmedian_arr = combined_arr.median(dim='time')\n\n\n#Outputs temporary median file \ntemp_median_file = outpath_aligned.joinpath(\"median_temp.tif\")\nmedian_arr.rio.to_raster(raster_path=temp_median_file)\n\n#Compresses temp median file into final median file\nmedian_file = outpath_aligned.joinpath(\"median_image.tif\")\ncmd = f\"gdal_translate -a_nodata 0 -co compress=LZW {temp_median_file} {median_file}\"\nsubprocess.check_output(cmd, shell=True)\nos.remove(temp_median_file)"
  },
  {
    "objectID": "tdi/tdi-notebook.html#stage-3-tophat-filtering-and-detection",
    "href": "tdi/tdi-notebook.html#stage-3-tophat-filtering-and-detection",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Stage 3: Tophat filtering and detection",
    "text": "Stage 3: Tophat filtering and detection\nThis section of the notebook performs the tophat filtering and outputs the final vehicle detections. If Stage 1 and 2 have already been run, start here.\n\nFunction definitions\n\ndef export_xarray(export_arr, ref_arr, xr_filename):\n    \"\"\"\n    Function used to convert a numpy array to an xarray and export it\n    \n    Params:\n    ---------------------\n    export_arr: numpy array\n        array to be exported\n    ref_arr: xarray data array\n        reference xarray to be used to set up the output\n    xr_filename: pathlib Path\n        filepath for the output file\n    \"\"\"\n\n    coords = {'x': ref_arr.coords['x'].to_numpy(), 'y': ref_arr.coords['y'].to_numpy()}\n    export_xr = xr.DataArray(export_arr, coords=coords, dims=('y', 'x'))\n\n    export_xr.rio.write_crs(4326, inplace=True)\n    export_xr.rio.to_raster(raster_path=xr_filename) \n\ndef get_kernel(size):\n    \"\"\"\n    Function defines kernels for multi-directional tophat filtering based on specified kernel size.\n\n    Params:\n    ---------------------\n    size: int\n        size for the tophat kernel. Options are 3, 5, and 7 currently.\n    \n    Returns:\n    ---------------------\n    kernel_dict: dict\n        dictionary with tophat kernels\n    \"\"\"\n    \n    if size == 3:\n        kernel_0 = np.array([[0,0,0],\n                             [1,1,1],\n                             [0,0,0]])\n        kernel_45 = np.array([[1,0,0],\n                              [0,1,0],\n                              [0,0,1]])\n        kernel_90 = np.array([[0,1,0],\n                              [0,1,0],\n                              [0,1,0]])\n        kernel_135 = np.array([[0,0,1],\n                               [0,1,0],\n                               [1,0,0]])\n    \n    if size == 5:\n        kernel_0 = np.array([[0,0,0,0,0],\n                             [0,0,0,0,0],\n                             [1,1,1,1,1],\n                             [0,0,0,0,0],\n                             [0,0,0,0,0]])\n        kernel_45 = np.array([[1,0,0,0,0],\n                              [0,1,0,0,0],\n                              [0,0,1,0,0],\n                              [0,0,0,1,0],\n                              [0,0,0,0,1]])\n        kernel_90 = np.array([[0,0,1,0,0],\n                              [0,0,1,0,0],\n                              [0,0,1,0,0],\n                              [0,0,1,0,0],\n                              [0,0,1,0,0]])\n        kernel_135 = np.array([[0,0,0,0,1],\n                               [0,0,0,1,0],\n                               [0,0,1,0,0],\n                               [0,1,0,0,0],\n                               [1,0,0,0,0]])\n    if size == 7:\n        kernel_0 = np.array([[0,0,0,0,0,0,0],\n                             [0,0,0,0,0,0,0],\n                             [0,0,0,0,0,0,0],\n                             [1,1,1,1,1,1,1],\n                             [0,0,0,0,0,0,0],\n                             [0,0,0,0,0,0,0],\n                             [0,0,0,0,0,0,0]])\n        kernel_45 = np.array([[1,0,0,0,0,0,0],\n                              [0,1,0,0,0,0,0],\n                              [0,0,1,0,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,0,1,0,0],\n                              [0,0,0,0,0,1,0],\n                              [0,0,0,0,0,0,1]])\n        kernel_90 = np.array([[0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0],\n                              [0,0,0,1,0,0,0]])\n        kernel_135 = np.array([[0,0,0,0,0,0,1],\n                               [0,0,0,0,0,1,0],\n                               [0,0,0,0,1,0,0],\n                               [0,0,0,1,0,0,0],\n                               [0,0,1,0,0,0,0],\n                               [0,1,0,0,0,0,0],\n                               [1,0,0,0,0,0,0]])\n\n    kernel_dict = {'kernel_0': kernel_0, 'kernel_45': kernel_45, 'kernel_90': kernel_90, 'kernel_135': kernel_135}\n\n    return kernel_dict\n\ndef calc_tophat_band(band_arr, kernel_size, tophat_type):\n    \"\"\"\n    Computes multiple tophat filters for a specific difference band (difference between target image\n    band and median image band). Returns the minimum tophat value from the multi-directional filtering.\n\n    Params:\n    ---------------------\n    band_arr: xarray data array\n        array for the specified band\n    kernel_size: int\n        size of kernel to use for tophat filtering\n    tophat_type: str\n        type of tophat filter to use. Options are \"white\" and \"black\"\n\n    Returns:\n    ---------------------\n    min_tophat: xarray data array\n        pixel-wise minimum of all tophat kernels\n    \"\"\"\n\n    kernel_dict = get_kernel(size=kernel_size)\n    \n    tophat_arrlist = []\n\n    for key, val in kernel_dict.items():\n        if tophat_type=='white':\n            tophat_arr = morphology.white_tophat(band_arr, footprint=val)\n        if tophat_type=='black':\n            tophat_arr = morphology.black_tophat(band_arr, footprint=val)\n        tophat_arrlist.append(tophat_arr)\n\n    tophat_combine = np.array(tophat_arrlist)\n    tophat_combine_coords = {'x': band_arr.coords['x'].to_numpy(), 'y': band_arr.coords['y'].to_numpy()}\n    tophat_combine_xr = xr.DataArray(tophat_combine, coords=tophat_combine_coords, dims=('band', 'y', 'x'))\n    min_tophat = tophat_combine_xr.min(dim='band')\n\n    return min_tophat\n\ndef calc_diff(band_arr, med_arr, method):\n    \"\"\"\n    Computes difference between band array from target image and corresponding\n    band from median image. Two methods are possible: simple pixel differencing or the\n    structural similarity index (ssim). Note that ssim was tested briefly but is not the \n    operational method.\n\n    Params:\n    ---------------------\n    band_arr: xarray data array\n        band array from target image\n    med_arr: xarray data array\n        band array from median image\n    method: str\n        Type of difference method to apply. Options are \"diff\" and \"ssim\"\n    \n    Returns:\n    ---------------------\n    diff_arr: numpy array\n        array of pixel differences between band_arr and med_arr\n    \"\"\"\n\n    if method == 'diff':\n        diff_arr = band_arr.to_numpy() - med_arr.to_numpy()\n    if method == 'ssim':\n        ssim_score, diff_arr = ssim(med_arr.to_numpy(), band_arr.to_numpy(), win_size=3, full=True)\n\n    return diff_arr\n\ndef run_tophat_filter(img_arr, median_arr, band_subset, diff_method, kernel_size):\n    \"\"\"\n    Function to perform tophat filtering on an image. Iterates through image bands, computes\n    difference between the target image band and the median image band, and runs tophat filtering\n    function (runs both \"black\" and \"white\" tophat filters). Once tophat filtering has been completed\n    for all bands, the maximum of all of the individual tophat filters for all bands is computed for each\n    tophat filter type (black and white).\n\n    Params:\n    ---------------------\n    img_arr: xarray data array\n        xarray data array for the target image\n    median_arr: xarray data array\n        xarray data array for the median image\n    band_subset: str\n        subset of image bands to use. Options are \"allbands\" to use all 8 superdove bands, or \n        \"4band\" to use only the BGRN bands.\n    diff_method: str\n        Type of difference method to apply. Options are \"diff\" and \"ssim\"\n    kernel_size: int\n        size of kernel to use for tophat filtering\n    \n    Returns:\n    ---------------------\n    tophat_dict: dict\n        dictionary containing numpy arrays for the two final tophat filters for the target image (black and white)\n    \"\"\"\n\n    tophat_typelist=['black', 'white']\n    band_index_dict = {'allbands': [0,1,2,3,4,5,6,7], '4band': [1,3,5,7]}\n    band_indices = band_index_dict[band_subset]\n\n    tophat_dict = {}\n    for tophat_type in tophat_typelist:\n        tophat_minlist = []\n        for i in range(0, len(img_arr)):\n            if i not in band_indices:\n                continue\n            band_xr = img_arr[i]\n            diff_arr = calc_diff(band_xr, median_arr[i], diff_method)\n            diff_xr = xr.DataArray(diff_arr, coords=band_xr.coords, dims=band_xr.dims, attrs=band_xr.attrs)\n            band_tophat = calc_tophat_band(diff_xr, kernel_size, tophat_type)\n            tophat_minlist.append(band_tophat)\n\n        min_tophat_combine_xr = xr.concat(tophat_minlist, dim='band')\n        tophat_final = min_tophat_combine_xr.max(dim='band')\n        tophat_dict[tophat_type] = tophat_final\n        \n        # tophat_finalfile = outpath_workdir.joinpath(f\"{select_date}_{diff_method}_method_tophat_{tophat_type}_kernel_{kernel_size}_{band_subset}.tif\")\n        # tophat_final.rio.write_crs(4326, inplace=True)\n        # tophat_final.rio.to_raster(raster_path=tophat_finalfile) \n\n    return tophat_dict\n\ndef combine_filters(tophat_white_arr, tophat_black_arr):\n    \"\"\"\n    Combine the black and white tophat filters for the target image into a single filter which is\n    the pixel-wise maximum of both.\n\n    Params:\n    ---------------------\n    tophat_white_arr: numpy array\n        white tophat filter array\n    tophat_black_arr: numpy array\n        black tophat filter array\n\n    Returns:\n    ---------------------\n    max_tophat: xarray data array\n        combined tophat filter array\n    \"\"\"\n    \n    combined_tophat = xr.concat([tophat_white_arr, tophat_black_arr], dim='band')\n    max_tophat = combined_tophat.max(dim='band')\n    max_tophat.rio.write_crs(4326, inplace=True)\n\n    return max_tophat\n\ndef clip_tophat(max_tophat, clip_geom):\n    \"\"\"\n    Clips tophat filter to road and parking area geometry\n\n    Params:\n    ---------------------\n    max_tophat: xarray data array\n        tophat filter array\n    clip_geom: geopandas geodataframe\n        geodataframe containing the clipping geometry\n\n    Returns:\n    ---------------------\n    clipped: xarray data array\n        clipped tophat filter array\n    \"\"\"\n\n    clipped = max_tophat.rio.clip(clip_geom.geometry.values, clip_geom.crs)\n\n    return clipped\n\ndef apply_threshold(max_tophat, threshold):\n    \"\"\"\n    Function to apply a threshold to a tophat filter array to create a boolean array.\n\n    Params:\n    ---------------------\n    max_tophat: xarray data array\n        tophat filter array\n    threshold: float\n        threshold to apply to tophat array\n\n    Returns:\n    tophat_bool: numpy array\n        boolean tophat array\n    ---------------------\n    \"\"\"\n    \n    tophat_bool = xr.where(max_tophat >= threshold, 1, 0)\n    tophat_bool = tophat_bool.astype(np.int8).to_numpy()\n    \n    return tophat_bool\n\ndef sieve_bool_filter(tophat_bool, sieve_thresh):\n    \"\"\"\n    Apply sieving to boolean tophat array to filter out small objects.\n\n    Params:\n    ---------------------\n    tophat_bool: numpy array\n        boolean tophat array\n    sieve_thresh: int\n        threshold to use for sieving. Objects smaller than the threshold are removed.\n\n    Returns:\n    ---------------------\n    tophat_sieve: numpy array\n        sieved boolean tophat array\n    \"\"\"\n    \n    labelled = label(tophat_bool)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        tophat_sieve = morphology.remove_small_objects(labelled, min_size=sieve_thresh, connectivity=1)\n        \n    tophat_sieve = np.where(tophat_sieve > 0, 1, 0)\n    tophat_sieve = tophat_sieve.astype(np.int8)\n    \n    return tophat_sieve\n\ndef threshold_tophat(clipped_tophat, min_thresh, sieve_thresh):\n    \"\"\"\n    Function to apply thresholding to the image tophat filters to create boolean detection arrays.\n    Initiallty uses the maximum value from the tophat array and iteratively reduces until it reaches \n    the specified minimum threshold value. After each iteration the results from the previous iteration \n    is subtracted from the current one, and sieving is applied to remove small objects. The sieved \n    filter array is then added together with the previous results to create a combined array.\n    \n    Note this iterative approach does not seem to add much value over just using the minimum value right \n    from the get-go, but does reduce the number of stray pixels and makes the detections possibly a bit more cohesive.\n    The iterative approach was implemented to adhere to the workflow outlined in Chen et al. 2021\n\n    Params:\n    ---------------------\n    clipped_tophat: xarray data array\n        clipped tophat filter array\n    min_thresh: float\n        minimum threshold value to use to create boolean vehicle detection array\n    sieve_thresh: int\n        minimum object size (in pixels). Objects smaller than the specified size are removed\n\n    Returns:\n    ---------------------\n    final_bool: numpy array\n        boolean vehicle detection array\n\n    \"\"\"\n    \n    max_thresh = clipped_tophat.max()\n    current_thresh = max_thresh\n    thresh_multiplier = 0.9\n\n    if current_thresh < min_thresh:\n        print(f'Max tophat value less than the specified minimum threshold setting: max tophat val = {max_thresh}; min threshold = {min_thresh}')\n        raise KeyboardInterrupt\n\n    first_iter = True\n    while current_thresh >= min_thresh:\n        if first_iter:\n            tophat_bool = apply_threshold(clipped_tophat, current_thresh)\n            tophat_bool = sieve_bool_filter(tophat_bool, sieve_thresh)\n            previous_bool = np.copy(tophat_bool)\n            first_iter = False\n        else:\n            tophat_bool = apply_threshold(clipped_tophat, current_thresh)\n            tophat_bool = tophat_bool - previous_bool\n            tophat_bool = sieve_bool_filter(tophat_bool, sieve_thresh)\n            previous_bool = previous_bool + tophat_bool\n        \n        current_thresh = current_thresh * thresh_multiplier\n\n    if current_thresh < min_thresh:\n        current_thresh = min_thresh\n        tophat_bool = apply_threshold(clipped_tophat, current_thresh)\n        tophat_bool = tophat_bool - previous_bool\n        tophat_bool = sieve_bool_filter(tophat_bool, sieve_thresh)\n        previous_bool = previous_bool + tophat_bool\n\n    final_bool = np.copy(previous_bool)\n\n    return final_bool\n\ndef calc_tdi(clipped_band, detection_arr):\n    \"\"\"\n    Calculates the Traffic Density Index (TDI) by dividing the number of vehicle detection pixels\n    by the total number of pixels within the road and parking lot geometry mask.\n\n    Params:\n    ---------------------\n    clipped_band: xarray data array\n        band from target image clipped to road geometry\n    detection_arr: numpy array\n        boolean vehicle detection array\n\n    Returns:\n    ---------------------\n    tdi: float\n        computed tdi value\n    \"\"\"\n\n    total_pixels = np.count_nonzero(clipped_band.to_numpy())\n    detection_pixels = np.count_nonzero(detection_arr)\n\n    tdi = (detection_pixels / total_pixels) * 100.0\n\n    return tdi\n\n\n\nTophat filtering code execution begins here\n\n#Read in median image file\nmedian_file = outpath_aligned.joinpath(\"median_image.tif\")\nmedian_arr = rioxr.open_rasterio(median_file)\n\n\n#Create output directory for the tophat rasters and boolean vehicle detection rasters\ntophat_outpath = outpath_workdir.joinpath(\"tophat_rasters\")\ntophat_outpath.mkdir(exist_ok=True)\n\ndetection_outpath = outpath_workdir.joinpath(\"detection_rasters\")\ndetection_outpath.mkdir(exist_ok=True)\n\n\n#To process a single image, set a date in the \"select_date\" variable. Otherwise leave it set to None to process all images\nselect_date = None\n# select_date = \"2022-12-24\"\n\n#Read in road clipping geometry\nroad_geom_gpd = gpd.read_file(road_geom_file)\n\n\nband_subset = 'allbands' #select subset of image bands or run all bands. Options are 'allbands' to use all bands, or '4band' to only process BGRN\ndiff_method = 'diff' #Method for comparing image with median image. 'diff' uses simple pixel differences, while 'ssim' uses the structural similarity index\nkernel_size=7 #size of kernel to use for tophat filtering. Options are 3, 5, and 7\nsieve_thresh = 5 #sieve threshold for filtering boolean detection arrays. Groups of detection pixels smaller than the threshold are removed\nmin_thresh=0.015 #minimum threshold used for vehicle detections from tophat filter array. Program will iteratively use smaller threshold values down to the value specified here.\n\ntdi_list = [] #list of TDI values. Each TDI value from each image is appended to this list\n\n#Iterate through all images in img_df and run vehicle detection and tdi calculation. If \"select_date\" variable set, will only process selected image\nfor index, content in img_df.iterrows():\n\n    img_name = content[\"product_name\"]\n    img_date = content['datestr']\n\n    if select_date and img_date != select_date:\n        continue\n\n    print(f'Calculating TDI for image: {img_name}')\n\n    img_file = outpath_aligned.joinpath(f\"{img_name}_aligned.tif\")\n    img_arr = rioxr.open_rasterio(img_file)\n    \n    #Create tophat filters for target image\n    tophat_dict = run_tophat_filter(img_arr, median_arr, band_subset, diff_method, kernel_size)\n\n    #Combine white and black tophat filters and save to file\n    max_tophat = combine_filters(tophat_dict['white'], tophat_dict['black'])\n    # tophat_combined_fname = outpath_workdir.joinpath(f\"{img_name}_{diff_method}_method_kernel_{kernel_size}_tophat.tif\")\n    tophat_tempname = tophat_outpath.joinpath(f\"{img_name}_tophat_temp.tif\")\n    tophat_combined_fname = tophat_outpath.joinpath(f\"{img_name}_tophat_raster.tif\")\n    max_tophat.rio.write_crs(4326, inplace=True)\n    max_tophat.rio.to_raster(raster_path=tophat_tempname) \n    cmd = f\"gdal_translate -co compress=LZW {tophat_tempname} {tophat_combined_fname}\"\n    subprocess.check_output(cmd, shell=True)\n\n    #Clip tophat array to road geometry\n    clipped_tophat = clip_tophat(max_tophat, road_geom_gpd)\n    \n    #Create boolean vehicle detection arry from clipped tophat array\n    final_bool = threshold_tophat(clipped_tophat, min_thresh, sieve_thresh)\n\n    #Output boolean vehicle detection array\n    # final_bool_fname = outpath_workdir.joinpath(f\"{img_name}_{diff_method}_method_kernel_{kernel_size}_minthresh_{min_thresh}_sieve_{sieve_thresh}.tif\")\n    bool_tempname = detection_outpath.joinpath(f\"{img_name}_vehicle_detection_temp.tif\")\n    final_bool_fname = detection_outpath.joinpath(f\"{img_name}_vehicle_detection_raster.tif\")\n    export_xarray(final_bool, clipped_tophat, bool_tempname)\n    cmd = f\"gdal_translate -a_nodata 0 -co compress=LZW {bool_tempname} {final_bool_fname}\"\n    subprocess.check_output(cmd, shell=True)\n\n    #Calculate TDI\n    clipped_img_band = img_arr[1].rio.clip(road_geom_gpd.geometry.values, road_geom_gpd.crs)\n    tdi = calc_tdi(clipped_img_band, final_bool)\n    tdi_list.append(tdi)\n\n    os.remove(tophat_tempname)\n    os.remove(bool_tempname)\n\n\n\nOutputting TDI to a CSV file and creating chart. This is the end of the current process\n\ntdi_df = img_df.copy()\n\ntdi_df['tdi'] = tdi_list\n\ntdi_pickle = outpath_workdir.joinpath(\"TDI_DF_2022-01-01_to_2023-03-27.pkl\")\ntdi_df.to_pickle(tdi_pickle)\n\ntdi_csv_file = outpath_workdir.joinpath(\"TDI_DF_2022-01-01_to_2023-03-27.csv\")\ntdi_df.to_csv(tdi_csv_file)\n\n\n\n# tdi_pickle = outpath_workdir.joinpath(\"TDI_DF_2022-01-01_to_2023-03-27.pkl\")\n# with open(tdi_pickle, \"rb\") as f:\n#     tdi_df = pickle.load(f)\n\n\n# tdi_df.nsmallest(5, 'tdi')\n\n\n# tdi_df.nlargest(5, 'tdi')\n\n\n#Creating TDI time series line chart\n\ntdi_fig = px.line(tdi_df, x='date', y='tdi', color_discrete_map={'tdi': 'red'})\ntdi_fig.update_traces(line_color='red', line_width=2)\n\nfignamestr = f\"TDI_Chart_2022-01-01_to_2023-03-27\"\ntdi_out_png = outpath_workdir.joinpath(f\"{fignamestr}.png\")\ntdi_out_html = outpath_workdir.joinpath(f\"{fignamestr}.html\")\ntdi_fig.write_image(tdi_out_png)\ntdi_fig.write_html(tdi_out_html)\ntdi_fig.show()"
  },
  {
    "objectID": "tdi/tdi-notebook.html#misc-testing-area-ignore-for-now",
    "href": "tdi/tdi-notebook.html#misc-testing-area-ignore-for-now",
    "title": "Sparkgeo - World Bank Demo",
    "section": "MISC TESTING AREA IGNORE FOR NOW",
    "text": "MISC TESTING AREA IGNORE FOR NOW\n\n# select_df = img_df.loc[img_df[\"datestr\"] == select_date].iloc[0]\n# img_name = select_df[\"product_name\"]\n# img_date = select_df['datestr']\n# img_file = outpath_aligned.joinpath(f\"{img_name}_aligned.tif\")\n# img_arr = rioxr.open_rasterio(img_file)\n\n\n# tophat_combined_fname = outpath_workdir.joinpath(f\"{select_date}_{diff_method}_method_kernel_{kernel_size}_combinedtophat.tif\")\n# max_tophat.rio.write_crs(4326, inplace=True)\n# max_tophat.rio.to_raster(raster_path=tophat_combined_fname) \n\n\n# bool_thresh = 0.01\n# sieve_thresh = 2\n# tophat_bool = threshold_filter(clipped_tophat, bool_thresh, sieve_thresh)\n# tophat_bool_fname = outpath_workdir.joinpath(f\"{select_date}_{diff_method}_method_kernel_{kernel_size}_boolthresh_{bool_thresh}_sieve_{sieve_thresh}_clipped.tif\")\n# export_xarray(tophat_bool, clipped_tophat, tophat_bool_fname)\n# tophat_bool.rio.write_crs(4326, inplace=True)\n# tophat_bool.rio.to_raster(raster_path=tophat_bool_fname)"
  },
  {
    "objectID": "tdi/tdi-notebook.html#radiometric-normalization-test-section",
    "href": "tdi/tdi-notebook.html#radiometric-normalization-test-section",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Radiometric normalization test section",
    "text": "Radiometric normalization test section\n\n## TESTING RAD NORMALIZATION\n# def create_rad_regmodel(band_arr, med_arr):\n#     diff_arr = np.abs(band_arr - med_arr)\n\n#     with warnings.catch_warnings():\n#         warnings.simplefilter(\"ignore\")\n#         percent_diff_arr = np.divide(diff_arr, med_arr) * 100.0\n\n#     percent_val = np.nanpercentile(percent_diff_arr, 25.0)\n#     low_diff_arr = np.where(percent_diff_arr > percent_val, np.nan, percent_diff_arr)\n\n#     band_arr_trunc = np.where(np.isnan(low_diff_arr), np.nan, band_arr)\n#     med_arr_trunc = np.where(np.isnan(low_diff_arr), np.nan, med_arr)\n\n#     x = med_arr_trunc.reshape((-1,1))\n#     y = band_arr_trunc.reshape((-1,1))\n#     x = x[~np.isnan(x)].reshape((-1,1))\n#     y = y[~np.isnan(y)].reshape((-1,1))\n\n#     x_ = PolynomialFeatures(degree=1, include_bias=False).fit_transform(x)\n#     model = LinearRegression().fit(x_, y)\n\n#     slope = round(float(model.coef_[0]), 5)\n#     intercept = round(float(model.intercept_[0]), 5)\n\n#     band_shape = band_arr.shape\n#     band_reshape = band_arr.reshape(-1,1)\n#     x_ = PolynomialFeatures(degree=1, include_bias=False).fit_transform(band_reshape)\n#     pred_flat = model.predict(x_)\n#     pred_arr = pred_flat.reshape(band_shape)\n\n#     return pred_arr\n#     # return slope, intercept\n#     # return model\n\n\n# band_indices = [0,1,2,3,4,5,6,7]\n\n# # df_list = []\n\n# for index, content in img_df.iterrows():\n#     if index != 91:\n#         continue\n    \n#     img_name = content[\"product_name\"]\n#     img_date = content['datestr']\n#     img_file = outpath_aligned.joinpath(f\"{img_name}_aligned.tif\")\n#     img_arr = rioxr.open_rasterio(img_file)\n\n#     # img_dict = {}\n#     # img_dict['img_name'] = img_name\n#     pred_arr_list = []\n#     for band_index in band_indices:\n#         band_arr = img_arr[band_index].to_numpy()\n#         med_arr = median_arr[band_index].to_numpy()\n\n#         pred_arr = create_rad_regmodel(band_arr, med_arr)\n\n#         pred_arr_list.append(pred_arr)\n\n#         # slope, intercept = create_rad_regmodel(band_arr, med_arr)\n\n#         # img_dict[f'b{band_index+1}_slope'] = slope\n#         # img_dict[f'b{band_index+1}_intercept'] = intercept\n\n#     # img_reg_df = pd.DataFrame(img_dict, index=[0], columns=['img_name', 'b1_slope', 'b2_slope', 'b3_slope', 'b4_slope', 'b5_slope', 'b6_slope', 'b7_slope', 'b8_slope', 'b1_intercept', 'b2_intercept', 'b3_intercept', 'b4_intercept', 'b5_intercept', 'b6_intercept', 'b7_intercept', 'b8_intercept'])\n#     # df_list.append(img_reg_df)\n\n#     radcor_arr = np.array(pred_arr_list)\n#     radcor_fname = outpath_workdir.joinpath(f\"{img_date}_radcor.tif\")\n    \n#     radcor_xr = xr.DataArray(radcor_arr, coords=img_arr.coords, dims=img_arr.dims)\n#     radcor_xr.rio.write_crs(4326, inplace=True)\n#     radcor_xr.rio.to_raster(raster_path=radcor_fname) \n\n# regression_df = pd.concat(df_list)\n# regression_df.reset_index(inplace=True, drop=True)\n\n\n# regression_df['b2_slope'].idxmax()\n\n\n# regression_df.iloc[91]"
  },
  {
    "objectID": "tdi/skywatch-api-notebook.html",
    "href": "tdi/skywatch-api-notebook.html",
    "title": "Sparkgeo - World Bank Demo",
    "section": "",
    "text": "import time\nimport warnings\nimport math\nfrom pathlib import Path\nimport requests\nimport pandas as pd\nimport geopandas as gpd\nimport json\nimport datetime\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pickle\nimport numpy as np\n\n\napi_key = \"\" #Put skywatch api key here\nheaders = {'Content-Type': 'application/json', 'x-api-key': api_key} #Used to construct queries. Do not change\n\n#Path to put all outputs\nout_path = Path(\"../data/raw/planetscope/\")\n#AOI geometry file used to query API\naoi_file = Path(\"../data/processed/Beitbridge_PS_AOI_Final.gpkg\")\naoi_data = gpd.read_file(aoi_file)\naoi_json = json.loads(aoi_data.to_json())\n\no_start_date = \"2023-01-01\" #Overall start date for query\no_end_date = \"2023-03-27\" #Overall end date for query\naoi_name = \"BeitBridge\" #Name for AOI"
  },
  {
    "objectID": "tdi/skywatch-api-notebook.html#search-api",
    "href": "tdi/skywatch-api-notebook.html#search-api",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Search API",
    "text": "Search API\n\ndate_interval = 90\n\no_start_datetime = datetime.datetime.strptime(o_start_date, \"%Y-%m-%d\")\no_end_datetime = datetime.datetime.strptime(o_end_date, \"%Y-%m-%d\")\nnum_intervals = math.floor((o_end_datetime - o_start_datetime).days / date_interval)\n\ndate_list = []\n\nif num_intervals > 0:\n    starting_date = None\n    for i in range(0, num_intervals):\n        if not starting_date:\n            starting_date = o_start_datetime\n        ending_date = starting_date + datetime.timedelta(days=date_interval)\n\n        date_list.append({\"start_date\": datetime.datetime.strftime(starting_date, \"%Y-%m-%d\"), \"end_date\": datetime.datetime.strftime(ending_date, \"%Y-%m-%d\")})\n        starting_date = ending_date + datetime.timedelta(days=1)\n\n    final_start_date = ending_date + datetime.timedelta(days=1)\n    date_list.append({\"start_date\": datetime.datetime.strftime(final_start_date, \"%Y-%m-%d\"), \"end_date\": o_end_date})\n\nelse:\n    date_list.append({\"start_date\": o_start_date, \"end_date\": o_end_date})\n\n\ndef get_sat_type(sat_id, source):\n    dove_r_list = [\"1047\", \"1057\", \"1058\", \"1059\", \"105a\", \"105b\", \"105d\", \"105e\", \"105f\", \"1060\", \"1061\", \"1062\", \"1063\", \"1064\", \"1065\", \"1066\", \"1067\", \"1068\", \"1069\", \"106a\", \"106b\", \"106c\", \"106d\", \"106e\", \"106f\"]\n\n    sat_parse = sat_id.split('_')[-1]\n    if source == \"PlanetScope-SuperDove\":\n        sat_type = 'superdove'\n    if source == \"PlanetScope\":\n        sat_type = 'dove-c'\n    \n    if any(sat in sat_parse for sat in dove_r_list):\n        sat_type = \"dove-r\"\n\n    return sat_type\n\ndef create_search_query(start_date, end_date, aoi_json):\n    query = {\n            \"location\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": aoi_json[\"features\"][0][\"geometry\"][\"coordinates\"][0]\n            },\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n            \"resolution\": [\"medium\"],\n            \"coverage\": 50.0,\n            \"interval_length\": 7,\n            \"order_by\": \"date\"\n        }\n    \n    return query\n\ndef search_catalog(query, headers):\n    search_url = \"https://api.skywatch.co/earthcache/archive/search\"\n    search_resp = requests.post(url=search_url, data=json.dumps(query), headers=headers).json()\n    search_id = search_resp['data']['id']\n    return search_id\n\ndef get_search_results(search_id, headers):\n    search_url = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results\"\n    search_resp = requests.get(url=search_url, headers=headers)\n\n    if search_resp.status_code == 202:\n        while search_resp.status_code == 202:\n            print(\"Search still processing. Waiting 5 seconds.\")\n            time.sleep(5)\n            search_resp = requests.get(url=search_url, headers=headers)\n    \n    return search_resp.json()\n\ndef parse_results(search_resp, headers, search_id, df_list):\n    for item in search_resp[\"data\"]:\n        datestr = item['start_time'].split(\"T\")[0]\n        date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n        year = date_obj.year\n        month = date_obj.month\n        isodate = date_obj.isocalendar()\n        if isodate[0] == (year - 1) and isodate[1] == 52:\n            week = 1\n        else:\n            week = isodate[1]\n        \n        sat_type = get_sat_type(item['product_name'], item['source'])\n        item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n                                \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n                                item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n                                \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n        df_list.append(item_df)\n    try:\n        cursor = search_resp['pagination']['cursor']['next']\n    except:\n        cursor = None\n    \n    while cursor:\n        search_url3 = f\"https://api.skywatch.co/earthcache/archive/search/{search_id}/search_results?cursor={cursor}\"\n        search_resp_2 = requests.get(url=search_url3, headers=headers).json()\n        for item in search_resp_2[\"data\"]:\n            datestr = item['start_time'].split(\"T\")[0]\n            date_obj = datetime.datetime.strptime(datestr, \"%Y-%m-%d\")\n            year = date_obj.year\n            month = date_obj.month\n            isodate = date_obj.isocalendar()\n            if isodate[0] == (year - 1) and isodate[1] == 52:\n                week = 1\n            else:\n                week = isodate[1]\n\n            sat_type = get_sat_type(item['product_name'], item['source'])\n            item_df = pd.DataFrame([{\"search_id\": search_id, \"id\": item['id'], \"product_name\": item['product_name'], \"datestr\": datestr, \n                                    \"date\": date_obj, \"year\": year, \"month\": month, \"week\": week, \"source\": item['source'], \"sat_type\": sat_type, \"area_sq_km\": \n                                    item['area_sq_km'], \"cloud_cover\": item['result_cloud_cover_percentage'], \"aoi_coverage\": item[\"location_coverage_percentage\"],\n                                    \"cost\": item['cost'], \"preview\": item[\"preview_uri\"]}])\n            df_list.append(item_df)\n        \n        try:\n            cursor = search_resp_2['pagination']['cursor']['next']\n        except:\n            cursor = None\n    \n    return df_list\n\n\ndf_list = []\nfor entry in date_list:\n    start_date = entry['start_date']\n    end_date = entry['end_date']\n    print(f\"running search query for date range: {start_date} to {end_date}\")\n    query = create_search_query(start_date, end_date, aoi_json)\n    search_id = search_catalog(query, headers)\n    search_resp = get_search_results(search_id, headers)\n\n    df_list = parse_results(search_resp, headers, search_id, df_list)\n\n\nsearch_df = pd.concat(df_list)\nsearch_df.reset_index(drop=True, inplace=True)\nsearch_df['year'] = search_df['year'].astype('category')\nsearch_df['month'] = search_df['month'].astype('category')\nsearch_df['week'] = search_df['week'].astype('category')\nsearch_df[\"preview_html\"] = search_df.apply(lambda x: f'<a href=\\\"{x[\"preview\"]}\\\">Preview</a>', axis=1)\n\n\nsearch_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}.pkl')\nsearch_df.to_pickle(search_pickle)"
  },
  {
    "objectID": "tdi/skywatch-api-notebook.html#filter-query-dataframe.-continue-from-here-if-using-previous-query",
    "href": "tdi/skywatch-api-notebook.html#filter-query-dataframe.-continue-from-here-if-using-previous-query",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Filter query Dataframe. Continue from here if using previous query",
    "text": "Filter query Dataframe. Continue from here if using previous query\n\nsearch_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}.pkl')\n\nwith open(search_pickle, \"rb\") as f:\n    search_df = pickle.load(f)\n\n\ndef filter_dates(df):\n    top_coverage = df.nlargest(1,'aoi_coverage')[\"aoi_coverage\"].tolist()[0]\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        df['cover_percentmax'] = df.apply(lambda row: (row[\"aoi_coverage\"] / top_coverage), axis=1)\n        \n    df = df.loc[(df[\"cover_percentmax\"]>=0.9)]\n\n    if 'superdove' in df['sat_type'].unique() or 'dove-r' in df['sat_type']:\n        df = df.loc[(df['sat_type'] == 'superdove') | (df['sat_type'] == 'dove-r')]\n\n    df = df.nlargest(1,'aoi_coverage')\n    select_id = df['id'].values[0]\n\n    return select_id\n\ndef filter_dataframe(df, cloud_cover_thresh, coverage_thresh):\n    filtered_df = df.copy(deep=True)\n    filtered_df = filtered_df.loc[(search_df[\"cloud_cover\"]<=cloud_cover_thresh) & (filtered_df[\"aoi_coverage\"] >= coverage_thresh)]\n\n    id_list = []\n    unique_dates = filtered_df.datestr.unique().tolist()\n    for date_ in unique_dates:\n        filtered_df2 = filtered_df.loc[filtered_df[\"datestr\"]==date_]\n        if len(filtered_df2) > 1:\n            select_id = filter_dates(filtered_df2)\n        else:\n            select_id = filtered_df2['id'].values[0]\n        \n        id_list.append(select_id)\n\n    filtered_df = filtered_df[filtered_df['id'].isin(id_list)]\n\n    filtered_df.reset_index(inplace=True, drop=True)\n    return filtered_df\n\ncc_thresh = 2.0\ncoverage_thresh = 80.0\nfiltered_search = filter_dataframe(search_df, cc_thresh, coverage_thresh)\n\n\nfilter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\nfiltered_search.to_pickle(filter_search_pickle)"
  },
  {
    "objectID": "tdi/skywatch-api-notebook.html#creating-graphs-to-explore-query-results",
    "href": "tdi/skywatch-api-notebook.html#creating-graphs-to-explore-query-results",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Creating graphs to explore query results",
    "text": "Creating graphs to explore query results\n\nfilter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\nwith open(filter_search_pickle, \"rb\") as f:\n    filtered_search = pickle.load(f)\n\n\ndef create_month_cc_graph(df, target_year, output_graph):\n    target_df = df.loc[df[\"year\"] == target_year]\n    min_date = target_df.iloc[0]['datestr']\n    max_date = target_df.iloc[-1]['datestr']\n\n    month_df = pd.DataFrame()\n    month_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['month'].value_counts()\n    month_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['month'].value_counts()\n    month_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['month'].value_counts()\n    month_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['month'].value_counts()\n    month_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['month'].value_counts()\n\n    month_df.reset_index(inplace=True)\n    month_df.rename(columns={'index': 'month'}, inplace=True)\n    month_df.sort_values('month', inplace=True)\n    month_df.reset_index(inplace=True, drop=True)\n\n    month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n    month_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Month - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n\n    filenamestr = f\"PlanetScope_Monthly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        month_fig.write_image(out_png)\n        month_fig.write_html(out_html)\n\n    month_fig.show()\n\ncreate_month_cc_graph(filtered_search, 2021, False)\n\n\ndef create_week_cc_graph(df, target_year, output_graph):\n    target_df = df.loc[df[\"year\"] == target_year]\n    min_date = target_df.iloc[0]['datestr']\n    max_date = target_df.iloc[-1]['datestr']\n    week_df = pd.DataFrame()\n    week_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['week'].value_counts()\n    week_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['week'].value_counts()\n    week_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['week'].value_counts()\n    week_df[\"<=10% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 5) & (target_df['cloud_cover'] <= 10)]['week'].value_counts()\n    week_df[\">10% clouds\"] = target_df.loc[target_df['cloud_cover'] > 10]['week'].value_counts()\n\n    week_df.reset_index(inplace=True)\n    week_df.rename(columns={'index': 'week'}, inplace=True)\n    week_df.sort_values('week', inplace=True)\n    week_df.reset_index(inplace=True, drop=True)\n\n    week_fig = px.bar(week_df, x='week', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n    week_fig.update_layout(title_text=f\"PlanetScope Cloud Cover By Week - {min_date} to {max_date} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Week of Year\")\n\n    filenamestr = f\"PlanetScope_Weekly_Image_Count_{min_date}_to_{max_date}_{aoi_name}\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        week_fig.write_image(out_png)\n        week_fig.write_html(out_html)\n\n    week_fig.show()\n\ncreate_week_cc_graph(filtered_search, 2020, False)\n\n\ndef create_image_preview_graph(df, output_graph):\n    layout = go.Layout(title=f'PlanetScope Images - {o_start_date} to {o_end_date} - {aoi_name}', xaxis=dict(title=\"Image Date\"), yaxis=dict(title=\"Cloud Cover Percentage\"))#, autosize=False, width=1200, height=510)\n\n    fig = go.Figure(layout=layout)\n\n    h_template='Image Date: %{customdata[0]}<br>%{customdata[3]}<br>Cloud Cover: %{customdata[2]}%<br>Image Source: %{customdata[1]}'\n\n    fig.add_trace(go.Scatter(\n        y=df['cloud_cover'],\n        x=df['date'],\n        mode='markers',\n        orientation='h',\n        marker=dict(\n            color='red'),\n        customdata=np.stack((df['datestr'], df['source'], df['cloud_cover'], df['preview_html']), axis=-1),\n        hovertemplate=h_template\n    ))\n\n    fig.update_layout(title={'x': 0.5, 'xanchor': 'center'})\n\n    if output_graph:\n        out_html = out_path.joinpath(f\"PlanetScope_Image_Previews_{o_start_date}_to_{o_end_date}_{aoi_name}.html\")\n        fig.write_html(out_html)\n\n    fig.show()\n\ncreate_image_preview_graph(filtered_search, False)\n\n\ndef create_year_cc_graph(df, output_graph):\n    target_df = df\n    min_year = target_df.iloc[0]['year']\n    max_year = target_df.iloc[-1]['year']\n    avg_price = target_df[\"cost\"].mean()\n    year_df = pd.DataFrame()\n    year_df[\"0% clouds\"] = target_df.loc[target_df['cloud_cover'] == 0]['year'].value_counts()\n    year_df[\"<=2% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 0) & (target_df['cloud_cover'] <= 2)]['year'].value_counts()\n    year_df[\"<=5% clouds\"] = target_df.loc[(target_df['cloud_cover'] > 2) & (target_df['cloud_cover'] <= 5)]['year'].value_counts()\n    year_df[\"0% clouds cost\"] = year_df.apply(lambda row: round((row[\"0% clouds\"] * avg_price), 2), axis=1)\n    year_df[\"<=2% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=2% clouds\"] * avg_price), 2), axis=1)\n    year_df[\"<=5% clouds cost\"] = year_df.apply(lambda row: round((row[\"<=5% clouds\"] * avg_price), 2), axis=1)\n\n    year_df.reset_index(inplace=True)\n    year_df.rename(columns={'index': 'year'}, inplace=True)\n    year_df.sort_values('year', inplace=True)\n    year_df.reset_index(inplace=True, drop=True)\n\n    h_template='Image Count 0% Clouds: %{customdata[0]} Estimated Cost: %{customdata[3]}<br>Image Count <=2% Clouds: %{customdata[1]} Estimated Cost: %{customdata[4]}<br>Image Count <=5% Clouds: %{customdata[2]} Estimated Cost: %{customdata[5]}'\n    year_fig = px.bar(year_df, x='year', y=['0% clouds', '<=2% clouds', '<=5% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\"}, custom_data=np.stack((year_df['0% clouds'], year_df['<=2% clouds'], year_df['<=5% clouds'], year_df['0% clouds cost'], year_df['<=2% clouds cost'], year_df['<=5% clouds cost'])))\n    year_fig.update_layout(title_text=f\"PlanetScope Annual Image Counts and Cost - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n    year_fig.update_traces(hovertemplate=h_template)\n\n    filenamestr = f\"PlanetScope_Annual_Image_Count_and_Cost_{min_year}_to_{max_year}_{aoi_name}\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        year_fig.write_image(out_png)\n        year_fig.write_html(out_html)\n\n    year_fig.show()\n\n    return year_df\n\nyear_df = create_year_cc_graph(filtered_search, False)\n\n\ndef create_month_multiyear_graph(df, output_graph):\n    \n    min_year = df.iloc[0]['year']\n    max_year = df.iloc[-1]['year']\n\n    month_df = pd.DataFrame()\n    col_list = []\n    for year in df.year.unique().tolist():\n        target_df = df.loc[df[\"year\"] == year]\n        col_name = f\"{year}_counts\"\n        month_df[col_name] = target_df.loc[target_df['cloud_cover'] == 0]['month'].value_counts()\n        col_list.append(col_name)\n\n    month_df.reset_index(inplace=True)\n    month_df.rename(columns={'index': 'month'}, inplace=True)\n    month_df.sort_values('month', inplace=True)\n    month_df.reset_index(inplace=True, drop=True)\n\n    # month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n    month_fig = px.bar(month_df, x='month', y=col_list, barmode='group')\n    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and Month - 0% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n\n    filenamestr = f\"PlanetScope_Image_Count_Annual_Monthly_0%CC_{min_year}_to_{max_year}_{aoi_name}\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        month_fig.write_image(out_png)\n        month_fig.write_html(out_html)\n\n    month_fig.show()\n\ncreate_month_multiyear_graph(filtered_search, False)\n\n\ndef create_dove_monthly_graph(df, target_year, cc_thresh, output_graph):\n    target_df = df.loc[df[\"year\"] == target_year]\n    min_date = target_df.iloc[0]['datestr']\n    max_date = target_df.iloc[-1]['datestr']\n\n    month_df = pd.DataFrame()\n    col_list = []\n    for datasource in df.sat_type.unique().tolist():\n        if datasource == \"dove-c\":\n            col_name = \"Dove-Classic\"\n        elif datasource == \"dove-r\":\n            col_name = \"Dove-R\"\n        elif datasource == \"superdove\":\n            col_name = \"SuperDove\"\n        \n        month_df[col_name] = target_df.loc[(target_df['cloud_cover'] == cc_thresh) & (target_df['sat_type'] == datasource)]['month'].value_counts()\n        col_list.append(col_name)\n\n    month_df.reset_index(inplace=True)\n    month_df.rename(columns={'index': 'month'}, inplace=True)\n    month_df.sort_values('month', inplace=True)\n    month_df.reset_index(inplace=True, drop=True)\n\n    # month_fig = px.bar(month_df, x='month', y=['0% clouds', '<=2% clouds', '<=5% clouds', '<=10% clouds', '>10% clouds'], color_discrete_map={'0% clouds': 'green','<=2% clouds': 'yellow', \"<=5% clouds\": \"orange\", \"<=10% clouds\": \"red\", \">10% clouds\": \"maroon\"})\n    month_fig = px.bar(month_df, x='month', y=col_list)\n    month_fig.update_layout(title_text=f\"PlanetScope Image Count By Month and By Satellite Type - {cc_thresh}% Cloud Cover - {target_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Month of Year\")\n\n    filenamestr = f\"PlanetScope_Image_Count_Monthly_{cc_thresh}%CC_SatelliteType_{target_year}_{aoi_name}\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        month_fig.write_image(out_png)\n        month_fig.write_html(out_html)\n\n    month_fig.show()\n\ncreate_dove_monthly_graph(filtered_search, 2021, 0, False)\n\n\ndef create_dove_year_graph(df, cc_thresh, output_graph):\n    target_df = df\n    min_year = df.iloc[0]['year']\n    max_year = df.iloc[-1]['year']\n    # avg_price = target_df[\"cost\"].mean()\n    year_df = pd.DataFrame()\n    year_df[\"Dove-Classic\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-c\")]['year'].value_counts()\n    year_df[\"Dove-R\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"dove-r\")]['year'].value_counts()\n    year_df[\"SuperDove\"] = target_df.loc[(target_df['cloud_cover'] <= 5) & (target_df[\"sat_type\"] == \"superdove\")]['year'].value_counts()\n    \n    year_df.reset_index(inplace=True)\n    year_df.rename(columns={'index': 'year'}, inplace=True)\n    year_df.sort_values('year', inplace=True)\n    year_df.reset_index(inplace=True, drop=True)\n\n    year_fig = px.bar(year_df, x='year', y=[\"Dove-Classic\", \"Dove-R\", \"SuperDove\"])\n    year_fig.update_layout(title_text=f\"PlanetScope Image Count By Year and By Satellite Type - {cc_thresh}% Cloud Cover - {min_year} to {max_year} - {aoi_name}\", title_x=0.5, yaxis_title=\"Image Count\", xaxis_title=\"Year\")\n\n    filenamestr = f\"PlanetScope_Annual_Image_Count_{cc_thresh}_%CC_SatelliteType_{min_year}_to_{max_year}_{aoi_name}_filtered\"\n    if output_graph:\n        out_png = out_path.joinpath(f\"{filenamestr}.png\")\n        out_html = out_path.joinpath(f\"{filenamestr}.html\")\n        year_fig.write_image(out_png)\n        year_fig.write_html(out_html)\n\n    year_fig.show()\n\n    return year_df\n\nyear_df = create_dove_year_graph(filtered_search, 0, False)\n# year_df = create_dove_year_graph(search_df, 5)"
  },
  {
    "objectID": "tdi/skywatch-api-notebook.html#download-imagery",
    "href": "tdi/skywatch-api-notebook.html#download-imagery",
    "title": "Sparkgeo - World Bank Demo",
    "section": "Download Imagery",
    "text": "Download Imagery\n\nfilter_search_pickle = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.pkl')\nwith open(filter_search_pickle, \"rb\") as f:\n    filtered_search = pickle.load(f)\n\n\n#Final image download selection criteria\nmax_cc = 0.0\n# start_date_download = o_start_date\n# end_date_download = o_end_date\nstart_date_download = \"2023-01-01\"\nend_date_download = \"2023-03-27\"\nstart_date_final = datetime.datetime.strptime(start_date_download, \"%Y-%m-%d\")\nend_date_final = datetime.datetime.strptime(end_date_download, \"%Y-%m-%d\")\nout_path = out_path.joinpath(out_path, f\"outputs_{aoi_name}_{start_date_download}_to_{end_date_download}\")\nout_path.mkdir(exist_ok=True)\nsat_list = ['superdove']\nfiltered_download_pickle = out_path.joinpath(f'search_df_{start_date_download}_to_{end_date_download}_download.pkl')\nfiltered_download_resp_pickle = out_path.joinpath(f'search_df_{start_date_download}_to_{end_date_download}_resp.pkl')\n\npl_resp_dict = {}\n\n\n\n#Filter results down to final download list based on above criteria\nfiltered_search_final = filtered_search.copy(deep=True)\nfiltered_search_final = filtered_search_final.loc[(filtered_search_final['date']>=start_date_final) & (filtered_search_final['date']<=end_date_final) & (filtered_search_final['cloud_cover']<=max_cc)& (filtered_search_final['sat_type'].isin(sat_list))]\n\nfiltered_search_final['pl_id'] = \"\"\nfiltered_search_final['pl_status'] = \"\"\n\nfiltered_search_final.to_pickle(filtered_download_pickle)\n\ntotal_cost = filtered_search_final['cost'].sum()\nnum_images = len(filtered_search_final)\nprint(f\"Total number of images to download: {num_images}. Total cost: ${total_cost} USD\")\n\n\n#Query endpoint to get different output types and build dictionary of selectable output types\noutput_id_url = f\"https://api.skywatch.co/earthcache/outputs\"\noutput_id_resp = requests.get(url=output_id_url, headers=headers).json()\noutput_type_dict = {}\nfor output in output_id_resp['data']:\n    output_name = output['name']\n    output_id = output['id']\n    output_type_dict[output_name] = output_id\n\noutput_type_dict.keys()\n\n\noutput_id = output_type_dict['All Optical Bands']\n\n\ndef create_pipeline_query(pipeline_name, search_id, image_id, output_id):\n    query = {\n            \"name\": pipeline_name,\n            \"search_id\": search_id,\n            \"search_results\": image_id,\n            \"output\": {\n                \"id\": output_id,\n                \"format\": \"geotiff\",\n                \"mosaic\": \"off\"\n            },\n            }\n    \n    return query\n\ndef post_pipeline(query, headers):\n    pipeline_url = \"https://api.skywatch.co/earthcache/pipelines\"\n    pipeline_resp = requests.post(url=pipeline_url, data=json.dumps(query), headers=headers).json()\n    pipeline_id = pipeline_resp['data']['id']\n    return pipeline_id\n\ndef get_pipeline(pipeline_id):\n    pipeline_get_url = f\"https://api.skywatch.co/earthcache/interval_results?pipeline_id={pipeline_id}\"\n    pipeline_get_resp = requests.get(url=pipeline_get_url, headers=headers)#.json()\n\n    return pipeline_get_resp\n\n\nfor index, row in filtered_search_final.iterrows():\n    time.sleep(1)\n    search_id = row[\"search_id\"]\n    image_id = row[\"id\"]\n    product_name = row[\"product_name\"]\n    print(f\"creating download pipeline for image: {product_name}\")\n    pipeline_name = f\"{aoi_name}_{product_name}\"\n    pl_query = create_pipeline_query(pipeline_name, search_id, image_id, output_id)\n    pl_id = post_pipeline(pl_query, headers)\n    filtered_search_final.loc[index, 'pl_id'] = pl_id\n\n\nfiltered_search_final.to_pickle(filtered_download_pickle)\n\nfilter_search_csv = out_path.joinpath(f'search_df_{o_start_date}_to_{o_end_date}_filtered.csv')\n\n\nif filtered_download_pickle.exists():\n    with open(filtered_download_pickle, 'rb') as f:\n        filtered_search_final = pickle.load(f)\nif filtered_download_resp_pickle.exists():\n    with open(filtered_download_resp_pickle, 'rb') as f:\n        pl_resp_dict = pickle.load(f)\n\n\ndef query_pipeline(pl_id):\n    pl_resp = get_pipeline(pl_id)\n    pl_resp_json = pl_resp.json()\n    pl_status = pl_resp_json['data'][0]['status']\n    return pl_status, pl_resp_json\n\nnum_iterations = 1\nresults_processing = True\nwhile results_processing:\n    status_list = []\n    print('Checking status of image download pipelines.')\n    for index, row in filtered_search_final.iterrows():\n        time.sleep(1)\n        pl_status = row['pl_status']\n        pl_id = row['pl_id']\n        if pl_status != 'complete':\n            pl_status, pl_resp_json = query_pipeline(pl_id)\n            filtered_search_final.loc[index, 'pl_status'] = pl_resp_json['data'][0]['status']\n            pl_resp_dict[pl_id] = pl_resp_json\n        \n        status_list.append(pl_status)\n\n    # if 'retrieving' not in set(status_list) and 'processing' not in set(status_list):\n    filtered_search_final.to_pickle(filtered_download_pickle)\n    with open(filtered_download_resp_pickle, 'wb') as f:\n        pickle.dump(pl_resp_dict, f)\n    if 'complete' in set(status_list) and len(set(status_list)) == 1:\n        results_processing = False\n        print('All image pipelines are finished and ready for download!')\n    else:\n        wait_time = 1800 / num_iterations\n        if wait_time < 60:\n            wait_time = 60\n        wait_time_mins = round((wait_time / 60), 0)\n        print(f\"Results still pending for some items. Waiting for {wait_time_mins} mins and trying again.\")\n        num_iterations += 2\n        time.sleep(wait_time)\n\n\ndef download_file(url, out_name):\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(out_name, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=None):\n                f.write(chunk)\n\nfor index, row in filtered_search_final.iterrows():\n    product_name = row['product_name']\n    pl_id = row['pl_id']\n    pl_resp_json = pl_resp_dict[pl_id]\n    results = pl_resp_json['data'][0]['results'][0]\n    download_dict = {'image': {'url': results['analytics_url'], 'extension': 'analytic.tif'},\n                    'metadata': {'url': results['metadata_url'], 'extension': 'metadata.json'},\n                    'cloud_mask': {'url': results['raster_files'][0]['uri'], 'extension': 'mask.tif'}}\n\n    out_basepath = out_path.joinpath(product_name)\n    out_basepath.mkdir(exist_ok=True)\n    print(f'downloading all files for image: {product_name}')\n    for key, value in download_dict.items():\n        dl_url = value['url']\n        extension = value['extension']\n        out_fname = out_basepath.joinpath(f'{product_name}_{extension}')\n        download_file(dl_url, out_fname)"
  },
  {
    "objectID": "Extract_Dates.html",
    "href": "Extract_Dates.html",
    "title": "Extract Important Dates",
    "section": "",
    "text": "This notebook is for extracting dates out of both the border wait time data and important dates for the Beitbridge border crossing. The important dates data set was generated through searching news articles, YouTube and other sources related to delays at the border crossing.\nFormat the important dates data to have start and end dates. Here we are splitting based on the arrow symbol. This is an artifact from Notion DB.\nFormat the border crossing dates as date time\nFunction to splice the data based on a date range for border data\nApply the function to our border data. Here were using the dates where we know there was an event and where we have high res imagery."
  },
  {
    "objectID": "Extract_Dates.html#plots",
    "href": "Extract_Dates.html#plots",
    "title": "Extract Important Dates",
    "section": "Plots",
    "text": "Plots\n\ndf %>%\n  ggplot(aes(x = Count_Events, y = Median_Minutes, group = Direction, col = Direction)) + \n  geom_point() +\n  scale_color_viridis_d(alpha=0.3,option = \"plasma\", end = .7) +\n  labs(y =\" Median Minutes\", x = \"Total Count\", title = \"Border Wait Time and Counts\", subtitle = \"December 24th, 2022\")\n\n\n\n\n\ndf %>%\n  ggplot(aes(x = StartHour, y = Median_Minutes, group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Hour of Day', y = 'Median Minutes', col=\"Direction\", title = 'December 24, 2022', caption = \"*Note missing data\")\n\n\n\n\n\ndf %>%\n  ggplot(aes(x = StartHour, y = Count_Events, group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Hour', y = 'Count', col=\"Direction\", title = 'December 24, 2022')"
  },
  {
    "objectID": "Extract_Dates.html#cumulative-sum",
    "href": "Extract_Dates.html#cumulative-sum",
    "title": "Extract Important Dates",
    "section": "Cumulative Sum",
    "text": "Cumulative Sum\n\ndf %>% \n  group_by(Direction)%>%\n  mutate(cumulative_count = cumsum(Count_Events))%>%\n  ggplot(aes(x = StartHour, y = cumulative_count , group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Hour', y = 'Cumulative Sum', col=\"Direction\", title = 'December 24, 2022')\n\n\n\n\n\ndf %>% \n  group_by(Direction)%>%\n  mutate(cumulative_count = cumsum(Count_Events))%>%\n  ggplot(aes(x = StartHour, y = cumulative_count , group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Hour', y = 'Cumulative Sum', col=\"Direction\", title = 'December 24, 2022')"
  },
  {
    "objectID": "Extract_Dates.html#month-plots",
    "href": "Extract_Dates.html#month-plots",
    "title": "Extract Important Dates",
    "section": "Month Plots",
    "text": "Month Plots\nThe plots below are looking at the month of December 2022.\n\nborder_data %>%\n    filter(year(StartDate)=='2022', month(StartDate)==12)%>%\n    group_by(Direction, StartDate)%>%\n    arrange(Direction, StartDate)%>%\n    summarize(daily_count = sum(Count_Events, na.rm=T))%>%\n    mutate(cumulative_sum =  cumsum(daily_count))%>%\n  ggplot(aes(x = StartDate, y = cumulative_sum , group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Date', y = 'Cumulative Sum', col=\"Direction\", title = 'December, 2022')\n\n\n\n\n\nborder_data %>%\n  filter(year(StartDate)=='2022', month(StartDate)==12)%>%\n  group_by(StartDate, Direction)%>%\n  summarize(max_median = max(Median_Minutes,na.rm=T))%>%\n  ggplot(aes(x=StartDate,y = max_median, group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  scale_x_date(date_labels = \"%d\",date_breaks = '2 days', limits =c(as_date('2022-12-01'),as_date('2022-12-31') )) +\n  theme(axis.text.x=element_text(angle=60, hjust=1)) +\n  labs(x= 'Day', y = 'Max Daily Median Wait Time', col=\"Direction\", title = 'December 2022')"
  },
  {
    "objectID": "Extract_Dates.html#hourly-scatter-plots",
    "href": "Extract_Dates.html#hourly-scatter-plots",
    "title": "Extract Important Dates",
    "section": "Hourly Scatter Plots",
    "text": "Hourly Scatter Plots\n\nborder_data %>%\n    filter(Direction == 'SA-Zimbabwe')%>%\n    filter(year(StartDate)=='2022', month(StartDate)==12)%>%\n    ggplot(aes(x=datetime, y=Median_Minutes, group=as_factor(Direction), col=as_factor(Direction)))+\n    geom_point(alpha = 0.7) +\n    #geom_line()+\n    labs(x= 'Day', y = 'Median Hourly Wait Time', col=\"Direction\", title = 'December 2022')+\n    scale_color_viridis_d(option = \"plasma\", end = .7)\n\n\n\n\n\nborder_data %>%\n    filter(Direction == 'Zimbabwe-SA')%>%\n    filter(year(StartDate)=='2022', month(StartDate)==12)%>%\n    ggplot(aes(x=datetime, y=Median_Minutes, group=as_factor(Direction), col=as_factor(Direction)))+\n    geom_point(alpha = 0.7) +\n    #geom_line()+\n    labs(x= 'Day', y = 'Median Hourly Wait Time', col=\"Direction\", title = 'December 2022')+\n    scale_color_viridis_d(option = \"magma\", end = .7)\n\n\n\n\n\nborder_data %>%\n  filter(year(StartDate)=='2022', month(StartDate)==12)%>%\n  group_by(StartDate, Direction)%>%\n  summarize(Total_Count= sum(Count_Events,na.rm=T))%>%\n  ggplot(aes(x=StartDate,y = Total_Count, group= as_factor(Direction), col = as_factor(Direction))) + \n  geom_point(alpha = 0.7) +\n  geom_line()+\n  scale_color_viridis_d(option = \"plasma\", end = .7)+\n  labs(x= 'Day', y = 'Total Count', col=\"Direction\", title = 'December 2022')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started",
    "section": "",
    "text": "Here youll find various notebooks created by Sparkgeo. Navigate to the notebooks on the left."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Getting Started",
    "section": "Installation",
    "text": "Installation\nFirst clone this repository to your local environment.\nTo run the notebooks locally you need to install the environments for R , Quarto and Python."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Getting Started",
    "section": "Python",
    "text": "Python\n\nConda or Mamba\n\nFor Building and TDI Notebooks\nTo build the environment from a yml file using mamba (recommended)\nmamba env create -f environment.yml\n\n\n\nVirtual Environment\n\nYOLO\nThe best way to run the notebooks and Python applications provided is to create a virtual environment with\nmkvirtualenv -p python3 worldbank\nInside the virtual environment, run\npip install -r requirements.txt\nThis should install all of the required Python modules into the virtual environment."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Getting Started",
    "section": "Usage",
    "text": "Usage\nFor the provided example applications, activate the virtual environment and then run one in this fashion:\npython src/count_cars.py --detection-file data/processed/test_cars.gpkg --intersect-file data/processed/beitbridge_road_mask_v2.gpkg --output-json cars.geojson\nPlease see the ReadMe in the notebooks directory for their usage.\n\nR Projects\nTo replicate the R environment, you can simply install.packages('renv') and open up the R Project file and run renv::restore() to restore the renv lockfile to load in all the appropriate libraries."
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Getting Started",
    "section": "Quarto",
    "text": "Quarto\nQuarto is an open-source scientific and technical publishing system.\nReview Quartos Quickstart for installation instructions.\nTo regenerate outputs from Quarto:\nquarto render hello.ipynb --to html\nquarto render hello.ipynb --to docx"
  },
  {
    "objectID": "MS_OSM_Data_download.html",
    "href": "MS_OSM_Data_download.html",
    "title": "Sparkgeo - World Bank Demo",
    "section": "",
    "text": "OSM Buildings\nCreate a graph from OSM using place_bbox for Beitbridge.\n\nG = ox.graph_from_bbox(place_bbox[0],place_bbox[1],place_bbox[2],place_bbox[3], network_type='drive', simplify=True, retain_all=False)\nfig, ax = ox.plot_graph(G)\n\n\n\n\nGet the building footprints from a bbox and plot them.\n\n#plot building footprint\nbuildings = ox.geometries_from_bbox(place_bbox[0],place_bbox[1],place_bbox[2],place_bbox[3],tags={'building': True})\nfig, ax = ox.plot_footprints(buildings, alpha=0.4, show=False)\nbuildings.plot(ax=ax, facecolor=\"blue\", alpha=0.7, zorder=2)\nplt.tight_layout()\nplt.axis(\"off\")\n\n(29.938105, 30.0540557, -22.2513996, -22.1488729)\n\n\n\n\n\n\n\nMS Buildings\nSearch against the Planetary Computer STAC API. You may need to register for an API key from planetary computer to make the following run:\n\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1/\",\n    modifier=planetary_computer.sign_inplace,\n)\n\nDefine your search with CQL2 syntax\n\nsearch = catalog.search(filter_lang=\"cql2-json\", filter={\n  \"op\": \"and\",\n  \"args\": [\n    {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, \"ms-buildings\"]},\n    {\"op\": \"=\", \"args\": [{\"property\": \"msbuildings:region\"}, place_name]},\n    {\n     \"op\": \">=\",\n     \"args\": [ { \"property\": \"end_datetime\" }, { \"timestamp\": \"2018-11-20T00:00:00+00:00\" } ]\n    }\n  ]\n})\n\nGrab the item from the search results\n\nitem = next(search.get_items())\nprint(item)\nprint(item.assets)\nasset = item.assets[\"data\"]\n\n<Item id=Zimbabwe_2022-07-06>\n{'data': <Asset href=abfs://footprints/global/2022-07-06/ml-buildings.parquet/RegionName=Zimbabwe>}\n\n\nConvert data into geopandas\n\ndf = geopandas.read_parquet(\n    asset.href,\n    storage_options=asset.extra_fields[\"table:storage_options\"],\n)\ndf=df.drop(columns=\"quadkey\")\ngdf=geopandas.GeoDataFrame(df, crs=\"EPSG:4326\")\nprint(df.head())\n\nImportError: Install adlfs to access Azure Datalake Gen2 and Azure Blob Storage\n\n\n\n# Clip the data using GeoPandas clip\nMS_clip = gpd.clip(gdf, polygon)\n\n\nMS_Buildings=geopandas.GeoDataFrame(MS_clip, crs=\"EPSG:4326\")\nMS_Buildings.head()\nMS_Buildings.to_file(filename=\"ms_buildings.geojson\", driver='GeoJSON')\n\n\nOSM_Buildings=geopandas.GeoDataFrame(buildings, crs=\"EPSG:4326\")\n\n\nOSM_Buildings.head()\nOSM_Buildings1 = OSM_Buildings[['geometry','building','nodes']]\n#OSM_Buildings.drop(columns=['osmid', 'layer', 'amenity', 'brand', 'name', 'shop',  'brand', 'roof:shape', 'ways', 'type'])\n\n\nwith open(\"./OSM_Buildings.geo.json\",mode=\"w\") as f:\n  geojson.dump(OSM_Buildings,f)"
  },
  {
    "objectID": "gps_modelling.html",
    "href": "gps_modelling.html",
    "title": "Beitbridge Time Series Modelling",
    "section": "",
    "text": "The following notebook is not complete.\n\nThe intention of this notebook is to attempt to model border wait time based on the border wait time data provided by World Bank. In the future, this was intended to be correlated and/or modelling with remote sensing derived traffic density index (TDI).\nTo approach this problem, we can use a time-series analysis technique. Since we are given data over three years, we can split it into training and testing datasets to evaluate the models performance. Here are the steps I would take:\n1. Data exploration and cleaning: Explore the dataset to check for any missing or inconsistent data. Also, visualize the data to understand its patterns and trends. This was done in our gps_eda.qmd notebook.\n2. Feature engineering: Create new features that could potentially help in predicting the wait time, such as day of the week, month, holidays, and any significant events that could impact border crossing.\n3. Time-series modeling: Since our data is time-series, we can use a time-series model to predict wait times. We can use various models such as Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA (SARIMA), or Prophet.\n4. Model validation: After training the model, we can use the test dataset to evaluate its performance. We can use metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) to evaluate the models accuracy.\n5. Visualization: Finally, we can visualize the predicted wait times against the actual wait times to understand the models performance visually."
  },
  {
    "objectID": "gps_modelling.html#caveats-and-uncertainties",
    "href": "gps_modelling.html#caveats-and-uncertainties",
    "title": "Beitbridge Time Series Modelling",
    "section": "Caveats and uncertainties:",
    "text": "Caveats and uncertainties:\n1. Corrupt area: Since there has been reports of corruption, we can not guarantee the authenticity of the data. There could be bribes to cross the border quicker than others.\n2. External factors: There could be external factors such as political instability, social unrest, or natural disasters, which could impact the border wait times. Since our dataset does not include these factors, our model might not perform well in predicting such events.\n3. Limited dataset: We only have three years of data, which might not be enough to capture long-term trends and patterns accurately. If there are significant changes in the border crossing policies or infrastructure, our model might not be able to capture such changes.\nOverall, while a time-series model could help predict border wait times, we need to be aware of the caveats and uncertainties that could impact the models accuracy."
  },
  {
    "objectID": "gps_modelling.html#read-in-the-libraries-and-data",
    "href": "gps_modelling.html#read-in-the-libraries-and-data",
    "title": "Beitbridge Time Series Modelling",
    "section": "Read in the libraries and data",
    "text": "Read in the libraries and data\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(caret)\nlibrary(mlbench)\n\n\nborder_data = read_csv(\"../data/processed/Beitbridge_Counts_Wait_Time_2018_2022.csv\")"
  },
  {
    "objectID": "gps_modelling.html#sa---zimbabwe",
    "href": "gps_modelling.html#sa---zimbabwe",
    "title": "Beitbridge Time Series Modelling",
    "section": "SA - Zimbabwe",
    "text": "SA - Zimbabwe\n\nsa_zimbabwe = border_data %>% filter(Direction == \"SA-Zimbabwe\")"
  },
  {
    "objectID": "gps_modelling.html#zimbabwe---sa",
    "href": "gps_modelling.html#zimbabwe---sa",
    "title": "Beitbridge Time Series Modelling",
    "section": "Zimbabwe - SA",
    "text": "Zimbabwe - SA\n\nzimbabwe_sa = border_data %>% filter(Direction == \"Zimbabwe-SA\")"
  },
  {
    "objectID": "gps_modelling.html#plotting",
    "href": "gps_modelling.html#plotting",
    "title": "Beitbridge Time Series Modelling",
    "section": "Plotting",
    "text": "Plotting\n\nConvert to time series\n\nborder_data_ts <- sa_zimbabwe \nborder_data_ts$datetime <- ymd_h(str_c(border_data_ts $StartDate, border_data_ts $StartHour))\nborder_data_ts = border_data_ts%>% select(datetime, Median_Minutes, Count_Events) %>% \n  as_tsibble(index = datetime)\n\n\ndaily_counts <- sa_zimbabwe%>%\n  group_by(date=date(StartDate))%>%\n  summarize(Daily_Counts = sum(Count_Events,na.rm=TRUE))%>%\n  ungroup()%>%\n  as_tsibble(index=date)\n\n\n\nMoving averages\nCreating moving averages for 5 days, 7 days and 14 days.\n\ndaily_counts = daily_counts %>% mutate(ma_5 = ma(Daily_Counts, 5), ma_7=ma(Daily_Counts,7), ma_14 = ma(Daily_Counts, 14))\n\n\ndaily_counts %>%\n  #filter(year(date)==2021)%>%\n  ggplot()+\n  geom_line(aes(x=date, y=Daily_Counts, col=\"Daily Counts\"))+\n  geom_line(aes(x=date, y=ma_14, col=\"14-ma\"))+\n  scale_colour_manual(\"Legend\",values = c(\"14-ma\" = \"blue\", \"Daily Counts\"=\"grey\")) +\n  labs(y = \"Daily Counts\", x = \"Date\")\n\n\n\n\nZoom in on year 2021\n\ndaily_counts %>%\n  filter(year(date)==2021)%>%\n  ggplot()+\n  geom_line(aes(x=date, y=Daily_Counts, col=\"Daily Counts\"))+\n  geom_line(aes(x=date, y=ma_14, col=\"14-ma\"))+\n  scale_colour_manual(\"Legend\",values = c(\"14-ma\" = \"blue\", \"Daily Counts\"=\"grey\")) +\n  labs(y = \"Daily Counts\", x = \"Date\")"
  },
  {
    "objectID": "gps_modelling.html#seasonal-decomposition",
    "href": "gps_modelling.html#seasonal-decomposition",
    "title": "Beitbridge Time Series Modelling",
    "section": "Seasonal Decomposition",
    "text": "Seasonal Decomposition\n\ndid not get to implement or test further due to budgetary constraints\n\n\nggAcf(daily_counts)"
  },
  {
    "objectID": "gps_modelling.html#caret---linear-model",
    "href": "gps_modelling.html#caret---linear-model",
    "title": "Beitbridge Time Series Modelling",
    "section": "Caret - Linear Model",
    "text": "Caret - Linear Model\n\nWe can create additional features from the GPS data like:\n\nDay of week\nMonth\nYear\nHour\n\n\n\n# Load the necessary libraries\n\n\n# Read in the data\nborder_data <- zimbabwe_sa\n#border_data<-read_csv('data.csv')\n# Convert date and hour columns to datetime format\nborder_data$datetime <- ymd_h(paste(border_data$StartDate, border_data$StartHour))\n\n# Create new features\nborder_data$day_of_week <- weekdays(border_data$datetime)\nborder_data$hour_of_day <- hour(border_data$datetime)\nborder_data$month <- month(border_data$datetime)\nborder_data$year <- year(border_data$datetime)\n# Split the data into training and testing sets\nset.seed(123)\nborder_data = border_data %>% filter(!is.na(Median_Minutes))\ntraining_indices <- createDataPartition(border_data$Median_Minutes, p = 0.8, list = FALSE)\nborder_data_train <- border_data[training_indices,]\nborder_data_test <- border_data[-training_indices,]\n\n# Define the model formula\nmodel_formula <- Median_Minutes~ day_of_week + hour_of_day + month + year + Count_Events\n\n# Train the model using caret\nborder_model <- train(model_formula, data = border_data_train, method = \"lm\")\n\n# Make predictions on the testing set\nborder_predictions <- predict(border_model, newdata = border_data_test)\n\n# Evaluate the model's performance using Mean Absolute Error\nmae <- mean(abs(border_predictions - border_data_test$Median_Minutes))\n\n# Print the MAE\nprint(mae)\n\n[1] 678.9349\n\n\n\nsummary(border_model)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1494   -605   -294     63  41854 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          -5.088e+05  1.504e+04 -33.828  < 2e-16 ***\nday_of_weekMonday    -2.519e+01  3.976e+01  -0.633   0.5264    \nday_of_weekSaturday   4.554e+01  3.920e+01   1.162   0.2454    \nday_of_weekSunday    -2.740e+01  3.919e+01  -0.699   0.4844    \nday_of_weekThursday   1.736e+01  3.905e+01   0.445   0.6566    \nday_of_weekTuesday   -6.136e+01  3.914e+01  -1.568   0.1170    \nday_of_weekWednesday -7.014e+00  3.913e+01  -0.179   0.8577    \nhour_of_day           1.600e+01  1.846e+00   8.670  < 2e-16 ***\nmonth                -7.094e+00  3.093e+00  -2.294   0.0218 *  \nyear                  2.523e+02  7.447e+00  33.874  < 2e-16 ***\nCount_Events          2.305e+01  3.662e+00   6.294 3.16e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1528 on 21174 degrees of freedom\nMultiple R-squared:  0.05729,   Adjusted R-squared:  0.05684 \nF-statistic: 128.7 on 10 and 21174 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "gps_modelling.html#random-forests",
    "href": "gps_modelling.html#random-forests",
    "title": "Beitbridge Time Series Modelling",
    "section": "Random Forests",
    "text": "Random Forests\n\n# Load the dataset\ndf = border_data\ndf = df %>% select(Count_Events, Median_Minutes,datetime,day_of_week,year)\n\n\n\n\n\n\n\nImportant\n\n\n\nThis will take awhile to run on a laptop! Beware of running below:\n\n\n\n# Create a train/test split\nset.seed(123)\ntrainIndex <- createDataPartition(df$Median_Minutes, p = .8, list = FALSE)\ntrain <- df[ trainIndex,]\ntest <- df[-trainIndex,]\n\n# Fit a random forest model\nset.seed(123)\nrf_model <- train(Median_Minutes ~ ., data = train, method = \"rf\")\n\n# Print the model results\nrf_model\n\nRandom Forest \n\n21185 samples\n    4 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 21185, 21185, 21185, 21185, 21185, 21185, ... \nResampling results across tuning parameters:\n\n  mtry  RMSE      Rsquared    MAE     \n  2     1487.270  0.09307127  697.2415\n  5     1546.652  0.06958624  713.6039\n  9     1677.811  0.05062974  758.2495\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n\nIt is important to note, however, that the performance of a random forest model for predicting median border wait time would depend on the quality and relevance of the data and features used to train the model. Additionally, it may be necessary to regularly retrain the model with new data to ensure that it remains accurate over time.\nTODO: - subset by years, model individual years to account for covid - use moving averages - interpolate missing median wait time values - try different ARIMA models, GAMs and Prophet library"
  },
  {
    "objectID": "gps_eda.html",
    "href": "gps_eda.html",
    "title": "Beitbridge GPS Exploration",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(lubridate)\nlibrary(tidymodels)\nlibrary(tidyverse)\nThis notebook is exploring the Beitbridge GPS data with plots and limited statistics. Some of these plots are throw-away."
  },
  {
    "objectID": "gps_eda.html#sa---zimbabwe",
    "href": "gps_eda.html#sa---zimbabwe",
    "title": "Beitbridge GPS Exploration",
    "section": "SA - Zimbabwe",
    "text": "SA - Zimbabwe\n\nsa_zimbabwe = beitbridge_border %>% filter(Direction == \"SA-Zimbabwe\")"
  },
  {
    "objectID": "gps_eda.html#day-of-week",
    "href": "gps_eda.html#day-of-week",
    "title": "Beitbridge GPS Exploration",
    "section": "Day of Week",
    "text": "Day of Week\nLets look at day of week across the study period.\n\nsa_zimbabwe %>%\n  ggplot(aes(x = Count_Events, y = Median_Minutes, group= as_factor(wday(StartDate, TRUE)), col = as_factor(wday(StartDate, TRUE)))) + \n  geom_point(alpha = 0.7) +\n  scale_color_viridis_d(\"Day of Week\", option = \"plasma\", end = .7, alpha =0.5)+\n  labs(x = \"Count\", y = \"Median Minutes\")"
  },
  {
    "objectID": "gps_eda.html#zimbabwe---sa",
    "href": "gps_eda.html#zimbabwe---sa",
    "title": "Beitbridge GPS Exploration",
    "section": "Zimbabwe - SA",
    "text": "Zimbabwe - SA\n\nzimbabwe_sa = beitbridge_border %>% filter(Direction == \"Zimbabwe-SA\")\n\n\nzimbabwe_sa %>%\n  filter(year(StartDate)=='2019')%>%\n  ggplot(aes(x = Count_Events, y = Median_Minutes, group = as_factor(year(StartDate)), col = as_factor(year(StartDate)))) + \n  geom_point(alpha = 0.7) +\n   # geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(\"Year\", option = \"plasma\", end = .7)+\n  labs(x = \"Count\", y = \"Median Minutes\")\n\n\n\n\nConvert to long format for plotting\n\nzim_long = zimbabwe_sa %>%\n  select(-Direction)%>%\n  pivot_longer(cols = c(Median_Minutes, Bottom_10__Minutes, Top_10__Minutes), values_to = 'minutes')\n\n\nzim_long %>%\n  filter(StartDate == '2018-01-01' & Count_Events > 0)%>%\n  mutate(name = str_replace(name,\"_\",\" \"))%>%\n   mutate(name = str_replace(name,\"__\",\" \"))%>%\n  ggplot(aes(x = StartHour, y = minutes, group = name, col = name))+\n  geom_point()+\n  geom_line() +\n  labs(x = \"Hour of Day\", y = \"Minutes\", col = \"Legend\")\n\n\n\n\n\n zimbabwe_sa %>%\n  filter(StartDate == '2022-12-30')%>%\n  ggplot(aes(x = StartHour, y = Median_Minutes), colour = 'blue')+\n  geom_point()+\n  geom_line(colour = 'blue')+\n  geom_ribbon(aes(ymin = Bottom_10__Minutes, ymax = Top_10__Minutes), fill = 'light blue', alpha = 0.4)+\n  labs(x = \"Hour\", y=\"Median Minutes\")"
  },
  {
    "objectID": "gps_eda.html#daily",
    "href": "gps_eda.html#daily",
    "title": "Beitbridge GPS Exploration",
    "section": "Daily",
    "text": "Daily\n\n zimbabwe_sa %>% group_by(StartDate) %>% summarize(Day_Total = sum(Count_Events))%>%ggplot(aes(x=StartDate, y=Day_Total))+geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "yolo.html",
    "href": "yolo.html",
    "title": "YOLO",
    "section": "",
    "text": "First we need to build the correct environment for our YOLO model.\nThe best way to run the notebooks and Python applications provided is to create a virtual environment with\nInside the virtual environment, run\nThis should install all of the required Python modules into the virtual environment."
  },
  {
    "objectID": "yolo.html#training-data",
    "href": "yolo.html#training-data",
    "title": "YOLO",
    "section": "Training Data",
    "text": "Training Data\nTraining data can be downloaded from their respective sources. Sparkgeo recommends xView for the object classes relevant to World Bank. Which you can find here"
  },
  {
    "objectID": "yolo.html#darknet",
    "href": "yolo.html#darknet",
    "title": "YOLO",
    "section": "Darknet",
    "text": "Darknet\nWith Darknet, you can train your own neural networks to recognize patterns and make predictions based on data. The framework is written in the C programming language, which means it is very fast and efficient. This makes it a popular choice for people who need to train neural networks on large datasets or in real-time applications.\nOne of the most well-known uses of Darknet is in the development of YOLO (You Only Look Once), an object detection algorithm that can identify multiple objects in an image with a single pass. YOLO has been used in a variety of applications, from self-driving cars to surveillance systems."
  },
  {
    "objectID": "yolo.html#yolo",
    "href": "yolo.html#yolo",
    "title": "YOLO",
    "section": "YOLO",
    "text": "YOLO\nYOLO (You Only Look Once) is a popular object detection algorithm that can detect multiple objects in an image with high accuracy and speed. It is widely used in applications such as autonomous vehicles, security systems, and augmented reality."
  },
  {
    "objectID": "yolo.html#usage",
    "href": "yolo.html#usage",
    "title": "YOLO",
    "section": "Usage",
    "text": "Usage\nFor the provided example applications, activate the virtual environment and then run one in this fashion:\npython src/count_cars.py --detection-file data/processed/test_cars.gpkg --intersect-file data/processed/beitbridge_road_mask_v2.gpkg --output-json cars.geojson\n\nDownload and install Darknet on your system. The Darknet website has a pre-built binary for Windows and instructions for building from source code on other platforms.\nAcquire a dataset of images and annotations. There are many public datasets available, such as COCO or VOC, that contain images with object annotations. Alternatively, you can create your own dataset by annotating objects in images using tools such as LabelImg.\nConvert the dataset into Darknet format. Darknet uses a specific format for image and annotation files, which can be converted using scripts provided with the framework or third-party tools.\nTrain the neural network using Darknet. This involves defining a configuration file that specifies the network architecture and training parameters, and running the darknet executable with the appropriate arguments to start training.\nEvaluate the trained network on test data. This involves running the darknet executable with arguments that specify the test data and network weights, and examining the output to evaluate the performance of the network.\n\nUse the trained network for object detection. This involves running the darknet executable with arguments that specify the image or video to process, and the trained network weights. The output will be a set of bounding boxes and class labels that identify the objects in the input image or video."
  },
  {
    "objectID": "gps_data.html",
    "href": "gps_data.html",
    "title": "Beitbridge Truck GPS Data",
    "section": "",
    "text": "This notebook is cleaning the GPS data obtained by World Bank for future modelling and analysis."
  },
  {
    "objectID": "gps_data.html#data-cleaning",
    "href": "gps_data.html#data-cleaning",
    "title": "Beitbridge Truck GPS Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nThis notebook is for checking data quality on the GPS data\nSubsetting the data for our Study Period (2018-2022)\nAdding zeros and na values for completenesss since not every hour or every day is represented consistently in this data."
  },
  {
    "objectID": "gps_data.html#load-libraries",
    "href": "gps_data.html#load-libraries",
    "title": "Beitbridge Truck GPS Data",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\n\n\n\n\nNote\n\n\n\nWell be using tidyverse packages to read in and manipulate our data.\n\nreadr is for reading in tabular data\nskimr provides a quick summary of tabular data\nlubridate is for working with date formats and time series\n\n\n\n\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(lubridate)\n\nRead in the raw data:\n\ngps_data = read_csv(\"../data/raw/Beitbridge_Border_2017_02_01_to_2023_02_28.csv\")\n\nRemove redundant field names and grand total from rest of table.\n\n#fix column names\ngps_data = gps_data %>%\n  rename_with(~ str_remove(., \"Border_Crossing_\"), everything())\n# remove grand total row\ngrand_total = gps_data[1,]\ngps_data =  gps_data[-1,]\n\nWe can use skim to get a quick overview of the data:\n\nskim(gps_data)\n\n\nData summary\n\n\nName\ngps_data\n\n\nNumber of rows\n68318\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nlogical\n1\n\n\nnumeric\n6\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nDirection\n0\n1\n11\n12\n0\n4\n0\n\n\nGeozoneName\n0\n1\n17\n17\n0\n1\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nIsGrandTotalRowTotal\n0\n1\n0\nFAL: 68318\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nProjectID\n0\n1\n1.30\n0.71\n1\n1.00\n1.00\n1.00\n3.0\n\n\n\nStartHour\n0\n1\n11.41\n5.96\n0\n7.00\n12.00\n16.00\n23.0\n\n\n\nCount_Events\n0\n1\n3.17\n2.77\n1\n1.00\n2.00\n4.00\n52.0\n\n\n\nMedian_Minutes\n0\n1\n1523.23\n1773.96\n10\n335.00\n1029.37\n2096.00\n42633.0\n\n\n\nBottom_10__Minutes\n0\n1\n1114.86\n1510.78\n10\n235.60\n634.70\n1394.60\n42633.0\n\n\n\nTop_10__Minutes\n0\n1\n2227.06\n2460.80\n10\n497.02\n1416.00\n3074.45\n53702.6\n\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nStartDate\n0\n1\n2017-02-01\n2023-02-28\n2019-07-28\n2210"
  },
  {
    "objectID": "gps_data.html#cut-the-data-for-our-study-period",
    "href": "gps_data.html#cut-the-data-for-our-study-period",
    "title": "Beitbridge Truck GPS Data",
    "section": "Cut the data for our Study Period",
    "text": "Cut the data for our Study Period\n\nRemove extra fields\nFormat StartDate as date\nReplace spaces between Direction field for consistency\nSplit our data set into two separate sets:\n\nsa_zimbabwe for those travelling in the SA > Zimbabwe Direction\nzimbabwe_sa for those travelling Zimbabwe > SA Direction\n\n\n\ngps_data = gps_data%>%\n  filter(between(year(StartDate), 2018, 2022))%>%\n  select(-c(IsGrandTotalRowTotal, GeozoneName, ProjectID))%>%\n  mutate(StartDate = as_date(StartDate),\n         Direction = str_replace_all(Direction,\" \",\"\"))\n\nsa_zimbabwe = gps_data %>%\n  filter(Direction == \"SA-Zimbabwe\")\n\nzimbabwe_sa = gps_data %>%\n  filter(Direction == \"Zimbabwe-SA\")"
  },
  {
    "objectID": "gps_data.html#data-quality",
    "href": "gps_data.html#data-quality",
    "title": "Beitbridge Truck GPS Data",
    "section": "Data Quality",
    "text": "Data Quality\n\nStart_hour - is not consistent need to add zeros to count events and other fields.\nCreate a new date_table containing all of the dates and hours for our study period.\n\n\n#YMD\n#Hours 0 to 23\n\nstart_date = ymd_hm(\"2018-01-01 00:00\")\nend_date = ymd_hm(\"2022-12-31 23:00\")\n\ndate_table = data.frame(StartDate = seq(start_date, end_date, by=\"hour\"))\n\ndate_table = date_table %>%\n  mutate(StartHour = hour(StartDate))%>%\n  mutate(StartDate = as_date(StartDate))\n\n\nJoin our gps data from each subset to our date table to fill in missing start hours and values with zero.\nReplace only Count_Events with zero and all na values applied to medians.\nWell do this for both sa_zimbabwe and zimbabwe_sa\n\n\nsa_zimbabwe = sa_zimbabwe %>%\n  full_join(date_table)%>%\n  mutate(Direction = \"SA-Zimbabwe\")%>%\n  mutate(across(c(Count_Events), ~replace_na(.x,0)))%>%\n  arrange(StartDate, StartHour)\n\n\nzimbabwe_sa = zimbabwe_sa %>%\n  full_join(date_table)%>%\n  mutate(Direction = \"Zimbabwe-SA\")%>%\n  mutate(across(c(Count_Events), ~replace_na(.x,0)))%>%\n  arrange(StartDate, StartHour)\n\nPut it back together into one data set.\n\nbeitbridge_border = bind_rows(zimbabwe_sa, sa_zimbabwe)\n\nSave the data to a new csv into our processed folder.\n\nwrite_csv(beitbridge_border, \"../data/processed/Beitbridge_Counts_Wait_Time_2018_2022.csv\")"
  }
]