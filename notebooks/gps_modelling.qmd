---
title: "Beitbridge Time Series Modelling"
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
execute:
  warning: false
  cache: true
---

The following notebook is not complete.\
\
The intention of this notebook is to attempt to model border wait time based on the border wait time data provided by World Bank. In the future, this was intended to be correlated and/or modelling with remote sensing derived traffic density index (TDI).

------------------------------------------------------------------------

To approach this problem, we can use a time-series analysis technique. Since we are given data over three years, we can split it into training and testing datasets to evaluate the model's performance. Here are the steps I would take:

1\. Data exploration and cleaning: Explore the dataset to check for any missing or inconsistent data. Also, visualize the data to understand its patterns and trends.

2\. Feature engineering: Create new features that could potentially help in predicting the wait time, such as day of the week, month, holidays, and any significant events that could impact border crossing.

3\. Time-series modeling: Since our data is time-series, we can use a time-series model to predict wait times. We can use various models such as Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA (SARIMA), or Prophet.

4\. Model validation: After training the model, we can use the test dataset to evaluate its performance. We can use metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) to evaluate the model's accuracy.

5\. Visualization: Finally, we can visualize the predicted wait times against the actual wait times to understand the model's performance visually.

Caveats and uncertainties:

1\. Corrupt area: Since the area is corrupt, we can not guarantee the authenticity of the data. There could be data manipulation or fabrication, which could impact the model's accuracy.

2\. External factors: There could be external factors such as political instability, social unrest, or natural disasters, which could impact the border wait times. Since our dataset does not include these factors, our model might not perform well in predicting such events.

3\. Limited dataset: We only have three years of data, which might not be enough to capture long-term trends and patterns accurately. If there are significant changes in the border crossing policies or infrastructure, our model might not be able to capture such changes.

Overall, while a time-series model could help predict border wait times, we need to be aware of the caveats and uncertainties that could impact the model's accuracy.

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(skimr)
library(lubridate)
library(tidymodels)
library(forecast)
library(tidyverse)
library(tsibble)
library(caret)
```

```{r}
border_data = read_csv("../data/processed/Beitbridge_Counts_Wait_Time_2018_2022.csv")
```

# Subsets

## SA - Zimbabwe

```{r}
sa_zimbabwe = border_data %>% filter(Direction == "SA-Zimbabwe")
```

## Zimbabwe - SA

```{r}
zimbabwe_sa = border_data %>% filter(Direction == "Zimbabwe-SA")
```

## Caret 

```{r}
# Load the necessary libraries


# Read in the data
border_data <- zimbabwe_sa
#border_data<-read_csv('data.csv')
# Convert date and hour columns to datetime format
border_data$datetime <- ymd_h(paste(border_data$StartDate, border_data$StartHour))

# Create new features
border_data$day_of_week <- weekdays(border_data$datetime)
border_data$hour_of_day <- hour(border_data$datetime)
border_data$month <- month(border_data$datetime)
border_data$year <- year(border_data$datetime)
# Split the data into training and testing sets
set.seed(123)
border_data = border_data %>% filter(!is.na(Median_Minutes))
training_indices <- createDataPartition(border_data$Median_Minutes, p = 0.8, list = FALSE)
border_data_train <- border_data[training_indices,]
border_data_test <- border_data[-training_indices,]

# Define the model formula
model_formula <- Median_Minutes~ day_of_week + hour_of_day + month + year + Count_Events

# Train the model using caret
border_model <- train(model_formula, data = border_data_train, method = "lm")

# Make predictions on the testing set
border_predictions <- predict(border_model, newdata = border_data_test)

# Evaluate the model's performance using Mean Absolute Error
mae <- mean(abs(border_predictions - border_data_test$Median_Minutes))

# Print the MAE
print(mae)
```


```{r}
summary(border_model)
```

## Random Forests 

::: {.callout-important}
This will take awhile to run on a laptop! 
Beware of running below:
:::


```{r}
library(caret)
library(mlbench)

# Load the dataset
df = border_data
df = df %>% select(Count_Events, Median_Minutes,datetime,day_of_week,year)
# Create a train/test split
set.seed(123)
trainIndex <- createDataPartition(df$Median_Minutes, p = .8, list = FALSE)
train <- df[ trainIndex,]
test <- df[-trainIndex,]

# Fit a random forest model
set.seed(123)
rf_model <- train(Median_Minutes ~ ., data = train, method = "rf")

# Print the model results
rf_model
```

It is important to note, however, that the performance of a random forest model for predicting median border wait time would depend on the quality and relevance of the data and features used to train the model. Additionally, it may be necessary to regularly retrain the model with new data to ensure that it remains accurate over time.

```{r}
df = border_data%>% 
  mutate(hour = factor(StartHour)) %>%
  pivot_wider(names_from = hour, values_from = Count_Events, names_prefix = "hour_")
```

```{r}
model <- glm(Count_Events ~ medi, data = border_data, family = "poisson")
```

```{r}
# View model summary
summary(model)

```

To do: 
- subset by years, model individual years to account for covid 
- use moving averages
- interpolate missing median wait time values 
- try different ARIMA models, GAMs and Prophet library